---
title: "Towards Automatic Face-to-Face Translation"
collection: publications
permalink: /publication/2019-10-21-10.1145/3343031.3351066
excerpt: 'In light of the recent breakthroughs in automatic machine translation systems, we propose a novel approach that we term as &quot;Face-to-Face Translation&quot;. As today&apos;s digital communication becomes increasingly visual, we argue that there is a need for systems that can automatically translate a video of a person speaking in language A into a target language B with realistic lip synchronization. In this work, we create an automatic pipeline for this problem and demonstrate its impact on multiple real-world applications. First, we build a working speech-to-speech translation system by bringing together multiple existing modules from speech and language. We then move towards &quot;Face-to-Face Translation&quot; by incorporating a novel visual module, LipGAN for generating realistic talking faces from the translated audio. Quantitative evaluation of LipGAN on the standard LRW test set shows that it significantly outperforms existing approaches across all standard metrics. We also subject our Face-to-Face Translation pipeline, to multiple human evaluations and show that it can significantly improve the overall user experience for consuming and interacting with multimodal content across languages.'
date: 2019-10-21
venue: 'ACM International Conference on Multimedia (ACM Multimedia)'
paperurl: 'https://arxiv.org/abs/2003.00418'
citation: 'Prajwal K R, Rudrabha Mukhopadhyay, Jerin Philip, Abhishek Jha, Vinay Namboodiri, and C V Jawahar. 2019. Towards Automatic Face-to-Face Translation. In Proceedings of the 27th ACM International Conference on Multimedia (MM &apos;19). Association for Computing Machinery, New York, NY, USA, 1428–1436. DOI:https://doi.org/10.1145/3343031.3351066'
---

<a href='https://arxiv.org/abs/2003.00418'>Download paper here</a>

In light of the recent breakthroughs in automatic machine translation systems, we propose a novel approach that we term as &quot;Face-to-Face Translation&quot;. As today&apos;s digital communication becomes increasingly visual, we argue that there is a need for systems that can automatically translate a video of a person speaking in language A into a target language B with realistic lip synchronization. In this work, we create an automatic pipeline for this problem and demonstrate its impact on multiple real-world applications. First, we build a working speech-to-speech translation system by bringing together multiple existing modules from speech and language. We then move towards &quot;Face-to-Face Translation&quot; by incorporating a novel visual module, LipGAN for generating realistic talking faces from the translated audio. Quantitative evaluation of LipGAN on the standard LRW test set shows that it significantly outperforms existing approaches across all standard metrics. We also subject our Face-to-Face Translation pipeline, to multiple human evaluations and show that it can significantly improve the overall user experience for consuming and interacting with multimodal content across languages.

Recommended citation: Prajwal K R, Rudrabha Mukhopadhyay, Jerin Philip, Abhishek Jha, Vinay Namboodiri, and C V Jawahar. 2019. Towards Automatic Face-to-Face Translation. In Proceedings of the 27th ACM International Conference on Multimedia (MM '19). Association for Computing Machinery, New York, NY, USA, 1428–1436. DOI:https://doi.org/10.1145/3343031.3351066