---
title: "Learning semantic sentence embeddings using sequential pair-wise discriminator"
collection: publications
permalink: /publication/2018-08-18-2018_coling
excerpt: 'In this paper, we propose a method for obtaining sentence-level embeddings. While the problem of securing word-level embeddings is very well studied, we propose a novel method for obtaining sentence-level embeddings. This is obtained by a simple method in the context of solving the paraphrase generation task. If we use a sequential encoder-decoder model for generating paraphrase, we would like the generated paraphrase to be semantically close to the original sentence. One way to ensure this is by adding constraints for true paraphrase embeddings to be close and unrelated paraphrase candidate sentence embeddings to be far. This is ensured by using a sequential pair-wise discriminator that shares weights with the encoder that is trained with a suitable loss function. Our loss function penalizes paraphrase sentence embedding distances from being too large. This loss is used in combination with a sequential encoder-decoder network. We also validated our method by evaluating the obtained embeddings for a sentiment analysis task. The proposed method results in semantic embeddings and outperforms the state-of-the-art on the paraphrase generation and sentiment analysis task on standard datasets. These results are also shown to be statistically significant.'
date: 2018-08-18
venue: 'International Conference on Computational Linguistics (COLING)'
paperurl: 'https://badripatro.github.io/Question-Paraphrases/'
citation: 'B.N. Patro, V.K. Kurmi, S. Kumar and V.P. Namboodiri, “Learning semantic sentence embeddings using sequential pair-wise discriminator”, <i>Proceedings of the 27th International Conference on Computational Linguistics, COLING 2018,</i> Santa Fe, New Mexico, USA, August 2018'
---

<a href='https://badripatro.github.io/Question-Paraphrases/'>Download paper here</a>

In this paper, we propose a method for obtaining sentence-level embeddings. While the problem of securing word-level embeddings is very well studied, we propose a novel method for obtaining sentence-level embeddings. This is obtained by a simple method in the context of solving the paraphrase generation task. If we use a sequential encoder-decoder model for generating paraphrase, we would like the generated paraphrase to be semantically close to the original sentence. One way to ensure this is by adding constraints for true paraphrase embeddings to be close and unrelated paraphrase candidate sentence embeddings to be far. This is ensured by using a sequential pair-wise discriminator that shares weights with the encoder that is trained with a suitable loss function. Our loss function penalizes paraphrase sentence embedding distances from being too large. This loss is used in combination with a sequential encoder-decoder network. We also validated our method by evaluating the obtained embeddings for a sentiment analysis task. The proposed method results in semantic embeddings and outperforms the state-of-the-art on the paraphrase generation and sentiment analysis task on standard datasets. These results are also shown to be statistically significant.

Recommended citation: B.N. Patro, V.K. Kurmi, S. Kumar and V.P. Namboodiri, “Learning semantic sentence embeddings using sequential pair-wise discriminator”, <i>Proceedings of the 27th International Conference on Computational Linguistics, COLING 2018,</i> Santa Fe, New Mexico, USA, August 2018