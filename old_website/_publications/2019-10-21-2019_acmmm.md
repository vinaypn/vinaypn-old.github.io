---
title: "Towards Automatic Face-to-Face Translation"
collection: publications
permalink: /publication/2019-10-21-2019_acmmm
excerpt: 'In light of the recent breakthroughs in automatic machine translation systems, we propose a novel approach that we term as &quot;Face-to-Face Translation&quot;. As today&apos;s digital communication becomes increasingly visual, we argue that there is a need for systems that can automatically translate a video of a person speaking in language A into a target language B with realistic lip synchronization. In this work, we create an automatic pipeline for this problem and demonstrate its impact in multiple real-world applications. First, we build a working speech-to-speech translation system by bringing together multiple existing modules from speech and language. We then move towards &quot;Face-to-Face Translation&quot; by incorporating a novel visual module, LipGAN for generating realistic talking faces from the translated audio. Quantitative evaluation of LipGAN on the standard LRW test set shows that it significantly outperforms existing approaches across all standard metrics. We also subject our Face-to-Face Translation pipeline, to multiple human evaluations and show that it can significantly improve the overall user experience for consuming and interacting with multimodal content across languages. Code, models and demo video are made publicly available.'
date: 2019-10-21
venue: '27th ACM International Conference on Multimedia (ACM-MM)'
paperurl: 'https://cvit.iiit.ac.in/research/projects/cvit-projects/facetoface-translation'
citation: 'Prajwal Renukanand*, Rudrabha Mukhopadhyay*, Jerin Philip, Abhishek Jha, Vinay Namboodiri and C.V. Jawahar, “Towards Automatic Face-to-Face Translation”, <i> 27th ACM International Conference on Multimedia (ACM-MM),</i> Nice, France, 2019, Pages 1428–1436'
---

<a href='https://cvit.iiit.ac.in/research/projects/cvit-projects/facetoface-translation'>Download paper here</a>

In light of the recent breakthroughs in automatic machine translation systems, we propose a novel approach that we term as &quot;Face-to-Face Translation&quot;. As today&apos;s digital communication becomes increasingly visual, we argue that there is a need for systems that can automatically translate a video of a person speaking in language A into a target language B with realistic lip synchronization. In this work, we create an automatic pipeline for this problem and demonstrate its impact in multiple real-world applications. First, we build a working speech-to-speech translation system by bringing together multiple existing modules from speech and language. We then move towards &quot;Face-to-Face Translation&quot; by incorporating a novel visual module, LipGAN for generating realistic talking faces from the translated audio. Quantitative evaluation of LipGAN on the standard LRW test set shows that it significantly outperforms existing approaches across all standard metrics. We also subject our Face-to-Face Translation pipeline, to multiple human evaluations and show that it can significantly improve the overall user experience for consuming and interacting with multimodal content across languages. Code, models and demo video are made publicly available.

Recommended citation: Prajwal Renukanand*, Rudrabha Mukhopadhyay*, Jerin Philip, Abhishek Jha, Vinay Namboodiri and C.V. Jawahar, “Towards Automatic Face-to-Face Translation”, <i> 27th ACM International Conference on Multimedia (ACM-MM),</i> Nice, France, 2019, Pages 1428–1436