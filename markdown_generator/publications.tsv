pub_date	title	venue	excerpt	citation	url_slug	paper_url
2004-10-01	Image retrieval based on projective invariance	IEEE International Conference on Image Processing (ICIP)	"We propose an image retrieval scheme based on projectively invariant features. Since cross-ratio is the fundamental invariant feature under projective transformations for points, we use that as the basic feature parameter. We compute the cross-ratios of point sets in quadruplets and a discrete representation of the distribution of the cross-ratio is obtained from the computed values. The distribution is used as the feature for retrieval purposes. The method is very effective in retrieving images, like buildings, having similar planar 3D structures."	"Rajashekhar, S. Chaudhuri and V. P. Namboodiri (2004). ""Image retrieval based on projective invariance."" <i>IEEE International Conference on Image Processing</i> Singapore, October 2004, Page 405-408"	2004_icip	https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1418776
2004-12-16	Use of Linear Diffusion in depth estimation based on defocus cue	"Fourth Indian Conference on Computer Vision, Graphics & Image Processing"	"Diffusion has been used extensively in computer vision. Most common applications of diffusion have been in low level vision problems like segmentation and edge detection. In this paper a novel application of the linear diffusion principle is made for the estimation of depth using the properties of the real aperture imaging system. The method uses two defocused images of a scene and the lens parameter setting as input and estimates the depth in the scene, and also generates the corresponding fully focused equivalent pin-hole image. The algorithm described here also brings out the equivalence of the two modalities, viz. depth from focus and depth from defocus for structure recovery."	"V.P. Namboodiri and S. Chaudhuri (2004). “Use of Linear Diffusion in depth estimation based on defocus cue” <i> Proceedings of Fourth Indian Conference on Computer Vision, Graphics & Image Processing (ICVGIP) </i> Kolkata, India. December 2004."	2004_icvgip	http://vinaypn.github.io/files/icvgip04.pdf
2005-06-20	Shock Filters based on Implicit Cluster Separation	IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)	One of the classic problems in low level vision is image restoration. An important contribution toward this effort has been the development of shock filters by Osher and Rudin (1990). It performs image deblurring using hyperbolic partial differential equations. In this paper we relate the notion of cluster separation from the field of pattern recognition to the shock filter formulation. A kind of shock filter is proposed based on the idea of gradient based separation of clusters. The proposed formulation is general enough as it can allow various models of density functions in the cluster separation process. The efficacy of the method is demonstrated through various examples.	"V.P. Namboodiri and S. Chaudhuri  (2005). ""Shock Filters based on Implicit Cluster Separation."" <i>Proc. of IEEE International Conference on Computer Vision and Pattern Recognition (CVPR),</i>San Diego June 2005, Page 1-6."	2005_cvpr	http://vinaypn.github.io/files/cvpr05.pdf
2006-12-13	Improved Kernel-Based Object Tracking Under Occluded Scenarios	"Fifth Indian Conference on Computer Vision, Graphics & Image Processing"	"A successful approach for object tracking has been kernel based object tracking [1] by Comaniciu et al.. The method provides an effective solution to the problems of representation and localization in tracking. The method involves representation of an object by a feature histogram with an isotropic kernel and performing a gradient based mean shift optimization for localizing the kernel. Though robust, this technique fails under cases of occlusion. We improve the kernel based object tracking by performing the localization using a generalized (bidirectional) mean shift based optimization. This makes the method resilient to occlusions. Another aspect related to the localization step is handling of scale changes by varying the bandwidth of the kernel. Here, we suggest a technique based on SIFT features [2] by Lowe to enable change of bandwidth of the kernel even in the presence of occlusion. We demonstrate the effectiveness of the techniques proposed through extensive experimentation on a number of challenging data sets."	"V.P. Namboodiri, A. Ghorawat and S. Chaudhuri (2006) “Improved Kernel-Based Object Tracking Under Occluded Scenarios”.<i> In: Kalra P.K., Peleg S. (eds) Computer Vision, Graphics and Image Processing. Lecture Notes in Computer Science, vol 4338.</i> Springer, Berlin, Heidelberg"	2006_icvgip	http://vinaypn.github.io/files/icvgip06.pdf
2007-01-01	Retrieval of images of man-made structures based on projective invariance	Pattern Recognition Journal	"In this paper we propose a geometry-based image retrieval scheme that makes use of projectively invariant features. Cross-ratio (CR) is an invariant feature under projective transformations for collinear points. We compute the CRs of point sets in quadruplets and the CR histogram is used as the feature for retrieval purposes. Being a geometric feature, it allows us to retrieve similar images irrespective of view point and illumination changes. We can retrieve the same building even if the facade has undergone a fresh coat of paints! Color and textural features can also be included, if desired. Experimental results show a favorably very good retrieval accuracy when tested on an image database of size 4000. The method is very effective in retrieving images having man-made objects rich in polygonal structures like buildings, rail tracks, etc."	"Rajashekhar, S. Chaudhuri and V.P. Namboodiri (2007), “Retrieval of images of man-made structures based on projective invariance”. <i>Pattern Recognition Journal, Volume 40, Issue 1, January 2007</i>, Pages 296-308"	2007_pr	https://www.sciencedirect.com/science/article/pii/S0031320306001671
2007-02-01	"On defocus, diffusion and depth estimation"	Pattern Recognition Letters	"An intrinsic property of real aperture imaging has been that the observations tend to be defocused. This artifact has been used in an innovative manner by researchers for depth estimation, since the amount of defocus varies with varying depth in the scene. There have been various methods to model the defocus blur. We model the defocus process using the model of diffusion of heat. The diffusion process has been traditionally used in low level vision problems like smoothing, segmentation and edge detection. In this paper a novel application of the diffusion principle is made for generating the defocus space of the scene. The defocus space is the set of all possible observations for a given scene that can be captured using a physical lens system. Using the notion of defocus space we estimate the depth in the scene and also generate the corresponding fully focused equivalent pin-hole image. The algorithm described here also brings out the equivalence of the two modalities, viz. depth from focus and depth from defocus for structure recovery."	"V.P. Namboodiri and S. Chaudhuri  (2007). “On defocus, diffusion and depth estimation” <i>Pattern Recognition Letters</i> Volume 28, Issue 3, 1 February 2007, Pages 311-319"	2007_prl	http://vinaypn.github.io/files/prl07.pdf
2007-05-30	Super-Resolution Using Sub-band Constrained Total Variation	International Conference on Scale Space and Variational Methods in Computer Vision (SSVM)	"Super-resolution of a single image is a severely ill-posed problem in computer vision. It is possible to consider solving this problem by considering a total variation based regularization framework. The choice of total variation based regularization helps in formulating an edge preserving scheme for super-resolution. However, this scheme tends to result in a piece-wise constant resultant image. To address this issue, we extend the formulation by incorporating an appropriate sub-band constraint which ensures the preservation of textural details in trade off with noise present in the observation. The proposed framework is extensively evaluated and the experimental results for the same are presented"	"P. Chatterjee,  V.P. Namboodiri, S. Chaudhuri (2007) “Super-Resolution Using Sub-band Constrained Total Variation” <i> In: Sgallari F., Murli A., Paragios N. (eds) Scale Space and Variational Methods in Computer Vision SSVM 2007.</i> Lecture Notes in Computer Science, vol 4485. Springer, Berlin, Heidelberg"	2007_ssvm	http://vinaypn.github.io/files/ssvm07.pdf
2007-09-10	Shape Recovery Using Stochastic Heat Flow	British Machine Vision Conference (BMVC)	"We consider the problem of depth estimation from multiple images based on the defocus cue. For a Gaussian defocus blur, the observations can be shown to be the solution of a deterministic but inhomogeneous diffusion process. However, the diffusion process does not sufficiently address the case in which the Gaussian kernel is deformed. This deformation happens due to several factors like self-occlusion, possible aberrations and imperfections in the aperture. These issues can be solved by incorporating a stochastic perturbation into the heat diffusion process. The resultant flow is that of an inhomogeneous heat diffusion perturbed by a stochastic curvature driven motion. The depth in the scene is estimated from the coefficient of the stochastic heat equation without actually knowing the departure from the Gaussian assumption. Further, the proposed method also takes into account the non-convex nature of the diffusion process. The method provides a strong theoretical framework for handling the depth from defocus problem."	"V.P. Namboodiri and S. Chaudhuri  (2007) “Shape Recovery Using Stochastic Heat Flow“<i> Proceedings of the British Machine Vision Conference 2007</i>, University of Warwick, UK, September 10-13, 2007"	2007_bmvc	http://vinaypn.github.io/files/bmvc07.pdf
2007-10-19	Image Restoration using Geometrically Stabilized Reverse Heat Equation	IEEE International Conference on Image Processing (ICIP)	"Blind restoration of blurred images is a classical ill-posed problem. There has been considerable interest in the use of partial differential equations to solve this problem. The blurring of an image has traditionally been modeled by Witkin [10] and Koenderink [4] by the heat equation. This has been the basis of the Gaussian scale space. However, a similar theoretical formulation has not been possible for deblurring of images due to the ill-posed nature of the reverse heat equation. Here we consider the stabilization of the reverse heat equation. We do this by damping the distortion along the edges by adding a normal component of the heat equation in the forward direction. We use a stopping criterion based on the divergence of the curvature in the resulting reverse heat flow. The resulting stabilized reverse heat flow makes it possible to solve the challenging problem of blind space varying deconvolution. The method is justified by a varied set of experimental results."	"V.P. Namboodiri and S. Chaudhuri  (2005). ""Image Restoration using Geometrically Stabilized Reverse Heat Equation."" <i>Proceedings of IEEE International Conference on Image Processing (ICIP)</i>, San Antonio, Texas, USA, 2007, Pages IV - 413 - 416."	2007_icip	http://vinaypn.github.io/files/icip07.pdf
2008-06-23	Recovery of relative depth from a single observation using an uncalibrated (real-aperture) camera	IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	"In this paper we investigate the challenging problem of recovering the depth layers in a scene from a single defocused observation. The problem is definitely solvable if there are multiple observations. In this paper we show that one can perceive the depth in the scene even from a single observation. We use the inhomogeneous reverse heat equation to obtain an estimate of the blur, thereby preserving the depth information characterized by the defocus. However, the reverse heat equation, due to its parabolic nature, is divergent. We stabilize the reverse heat equation by considering the gradient degeneration as an effective stopping criterion. The amount of (inverse) diffusion is actually a measure of relative depth. Because of ill-posedness we propose a graph-cuts based method for inferring the depth in the scene using the amount of diffusion as a data likelihood and a smoothness condition on the depth in the scene. The method is verified experimentally on a varied set of test cases."	"V.P. Namboodiri and S. Chaudhuri  (2008). ""Recovery of relative depth from a single observation using an uncalibrated (real-aperture) camera."" <i>Proc. of IEEE International Conference on Computer Vision and Pattern Recognition (CVPR),</i>Anchorage, AK, USA, June 2008, Page 1-6."	2008_cvpr	http://vinaypn.github.io/files/cvpr08.pdf
2008-10-12	Regularized depth from defocus	IEEE International Conference on Image Processing (ICIP)	"n the area of depth estimation from images an interesting approach has been structure recovery from defocus cue. Towards this end, there have been a number of approaches [4,6]. Here we propose a technique to estimate the regularized depth from defocus using diffusion. The coefficient of the diffusion equation is modeled using a pair-wise Markov random field (MRF) ensuring spatial regularization to enhance the robustness of the depth estimated. This framework is solved efficiently using a graph-cuts based techniques. The MRF representation is enhanced by incorporating a smoothness prior that is obtained from a graph based segmentation of the input images. The method is demonstrated on a number of data sets and its performance is compared with state of the art techniques."	"V.P. Namboodiri, S. Chaudhuri and S. Hadap (2008). “Regularized depth from defocus”, <i> Proceedings of IEEE International Conference on Image Processing  (ICIP) </i>, San Diego, CA, USA, pp. 1520-1523."	2008_icip	http://vinaypn.github.io/files/icip08.pdf
2011-01-05	Action Recognition: A Region Based Approach	 IEEE Workshop on Applications of Computer Vision (WACV)	"We address the problem of recognizing actions in reallife videos. Space-time interest point-based approaches have been widely prevalent towards solving this problem. In contrast, more spatially extended features such as regions have not been so popular. The reason is, any local region based approach requires the motion flow information for a specific region to be collated temporally. This is challenging as the local regions are deformable and not well delineated from the surroundings. In this paper we address this issue by using robust tracking of regions and we show that it is possible to obtain region descriptors for classification of actions. This paper lays the groundwork for further investigation into region based approaches. Through this paper we make the following contributions a) We advocate identification of salient regions based on motion segmentation b) We adopt a state-of-the art tracker for robust tracking of the identified regions rather than using isolated space-time blocks c) We propose optical flow based region descriptors to encode the extracted trajectories in piece-wise blocks. We demonstrate the performance of our system on real-world data sets."	"H. Bilen, V.P. Namboodiri and L. Van Gool (2011). “Action recognition: A region based approach”, <i> 2011 IEEE Workshop on Applications of Computer Vision (WACV), Kona, HI</i> , 2011, pp. 294-300"	2011_wacv	http://vinaypn.github.io/files/wacv2011.pdf
2011-09-02	Object and Action Classification with Latent Variables	British Machine Vision Conference	In this paper we propose a generic framework to incorporate unobserved auxiliary information for classifying objects and actions. This framework allows us to explicitly account for localisation and alignment of representations for generic object and action classes as latent variables. We approach this problem in the discriminative setting as learning a max-margin classifier that infers the class label along with the latent variables. Through this paper we make the following contributions a) We provide a method for incorporating latent variables into object and action classification b) We specifically account for the presence of an explicit class related subregion which can include foreground and/or background. c) We explore a way to learn a better classifier by iterative expansion of the latent parameter space. We demonstrate the performance of our approach by rigorous experimental evaluation on a number of standard object and action recognition datasets. <br> <b> Awarded: Best Paper Prize </b>	"H. Bilen, V.P. Namboodiri and L. Van Gool (2011). “Object and Action Classification with Latent Variables”, <i> In Jesse Hoey, Stephen McKenna and Emanuele Trucco, Proceedings of the British Machine Vision Conference</i>, pages 17.1-17.11. BMVA Press, September 2011"	2011_bmvc	http://vinaypn.github.io/files/bmvc2011.pdf
2011-09-22	Super-resolution techniques for minimally invasive surgery	MICCAI workshop on augmented environments for computer assisted interventions-AE-CAI	"We propose the use of super-resolution techniques to aid visualization while carrying out minimally invasive surgical procedures. These procedures are performed using small endoscopic cameras, which inherently have limited imaging resolution. The use of higher-end cam- eras is technologically challenging and currently not yet cost effective. A promising alternative is to consider improving the resolution by post- processing the acquired images through the use of currently prevalent super-resolution techniques. In this paper we analyse the different method- ologies that have been proposed for super-resolution and provide a comprehensive evaluation of the most significant algorithms. The methods are evaluated using challenging in-vivo real world medical datasets. We suggest that the use of a learning-based super-resolution algorithm com- bined with an edge-directed approach would be most suited for this application. "	"V. De Smet, V.P. Namboodiri and L. Van Gool (2011). “Super-resolution techniques for minimally invasive surgery”, <i> 6th MICCAI workshop on augmented environments for computer assisted interventions-AE-CAI 2011 </i>, Toronto,Canada."	2011_aecai	http://vinaypn.github.io/files/AECAI.pdf
2011-11-06	Systematic evaluation of super-resolution using classification	 Visual Communications and Image Processing (VCIP)	"Currently two evaluation methods of super-resolution (SR) techniques prevail: The objective Peak Signal to Noise Ratio (PSNR) and a qualitative measure based on manual visual inspection. Both of these methods are sub-optimal: The latter does not scale well to large numbers of images, while the former does not necessarily reflect the perceived visual quality. We address these issues in this paper and propose an evaluation method based on image classification. We show that perceptual image quality measures like structural similarity are not suitable for evaluation of SR methods. On the other hand a systematic evaluation using large datasets of thousands of real-world images provides a consistent comparison of SR algorithms that corresponds to perceived visual quality. We verify the success of our approach by presenting an evaluation of three recent super-resolution algorithms on standard image classification datasets."	"V. De Smet, V.P. Namboodiri and L. Van Gool (2011). “Systematic evaluation of super-resolution using classification”, <i> 2011 Visual Communications and Image Processing (VCIP), Tainan</i>, 2011, pp. 1-4."	2011_vcip	http://vinaypn.github.io/files/VCIP.pdf
2012-08-28	"Classification with Global, Local and Shared Features"	Joint DAGM (German Association for Pattern Recognition) and OAGM Symposium	"We present a framework that jointly learns and then uses multiple image windows for improved classification. Apart from using the entire image content as context, class-specific windows are added, as well as windows that target class pairs. The location and extent of the windows are set automatically by handling the window parameters as latent variables. This framework makes the following contributions: a) the addition of localized information through the class-specific windows improves classification, b) windows introduced for the classification of class pairs further improve the results, c) the windows and classification parameters can be effectively learnt using a discriminative max-margin approach with latent variables, and d) the same framework is suited for multiple visual tasks such as classifying objects, scenes and actions. Experiments demonstrate the aforementioned claims."	"Bilen H., Namboodiri V.P., Van Gool L.J. (2012) Classification with Global, Local and Shared Features. In: Pinz A., Pock T., Bischof H., Leberl F. (eds) <i>Joint DAGM (German Association for Pattern Recognition) and OAGM Symposium.</i> Lecture Notes in Computer Science, vol 7476. Springer, Berlin, Heidelberg, pp 134-143"	2012_dagm	http://vinaypn.github.io/files/dagm2012.pdf
2013-01-15	Nonuniform Image Patch Exemplars for Low Level Vision	IEEE Workshop on Applications of Computer Vision (WACV)	"We approach the classification problem in a discrim- inative setting, as learning a max-margin classifier that infers the class label along with the latent variables. Through this paper we make the following contribu- tions: a) we provide a method for incorporating latent variables into object and action classification; b) these variables determine the relative focus on foreground vs. background information that is taken account of; c) we design an objective function to more effectively learn in unbalanced data sets; d) we learn a better classifier by iterative expansion of the latent parameter space. We demonstrate the performance of our approach through"	"V. De Smet, L. Van Gool and V. P. Namboodiri, ""Nonuniform image patch exemplars for low level vision,"" <i>2013 IEEE Workshop on Applications of Computer Vision (WACV)</i>, Tampa, FL, 2013, pp. 23-30."	2013_wacv	http://vinaypn.github.io/files/wacv2013.pdf
2014-02-01	Object and Action Classification with Latent Window Parameters	International Journal of Computer Vision (IJCV)	"In this paper we propose a generic framework to incorporate unobserved auxiliary information for classifying objects and actions. This framework allows us to automatically select a bounding box and its quadrants from which best to extract features. These spatial subdivisions are learnt as latent variables. The paper is an extended version of our earlier work [2], complemented with additional ideas, experiments and analysis. <br> We approach the classification problem in a discriminative setting, as learning a max-margin classifier that infers the class label along with the latent variables. Through this paper we make the following contributions: a) we provide a method for incorporating latent variables into object and action classification; b) these variables determine the relative focus on foreground vs. background information that is taken account of; c) we design an objective function to more effectively learn in unbalanced data sets; d) we learn a better classifier by iterative expansion of the latent parameter space. We demonstrate the performance of our approach through experimental evaluation on a number of standard object and action recognition data sets."	"H. Bilen, V.P. Namboodiri and L. Van Gool (2014), “Object and Action Classification with Latent Window Parameters”, <i>International Journal of Computer Vision (IJCV)</i> Vol: 106: 237 - 251, February 2014 "	2014_ijcv	http://vinaypn.github.io/files/ijcv2014.pdf
2014-06-23	Object classification with adaptable regions	IEEE Conference on Computer Vision and Pattern Recognition	"In classification of objects substantial work has gone into improving the low level representation of an image by considering various aspects such as different features, a number of feature pooling and coding techniques and considering different kernels. Unlike these works, in this paper, we propose to enhance the semantic representation of an image. We aim to learn the most important visual components of an image and how they interact in order to classify the objects correctly. To achieve our objective, we propose a new latent SVM model for category level object classification. Starting from image-level annotations, we jointly learn the object class and its context in terms of spatial location (where) and appearance (what). Furthermore, to regularize the complexity of the model we learn the spatial and co-occurrence relations between adjacent regions, such that unlikely configurations are penalized. Experimental results demonstrate that the proposed method can consistently enhance results on the challenging Pascal VOC dataset in terms of classification and weakly supervised detection. We also show how semantic representation can be exploited for finding similar content."	"H. Bilen, M. Pedersoli, V. P. Namboodiri, T. Tuytelaars, L. Van Gool,“Object Classification with Adaptable Regions”, Proceedings of IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), 2014"	2014_cvpr	http://vinaypn.github.io/files/cvpr2014.pdf
2014-12-13	Mind the gap: Subspace based hierarchical domain adaptation	"Workshop in Transfer and Multi-View Learning in Advances in Neural Information System Conference (NIPS) 27, "	"Domain adaptation techniques aim at adapting a classifier learnt on a source domain to work on the target domain. Exploiting the subspaces spanned by features of the source and target domains respectively is one approach that has been investigated towards solving this problem. These techniques normally assume the existence of a single subspace for the entire source / target domain. In this work, we consider the hierarchical organization of the data and consider multiple subspaces for the source and target domain based on the hierarchy. We evaluate different subspace based domain adaptation techniques under this setting and observe that using different subspaces based on the hierarchy yields consistent improvement over a non-hierarchical baseline"	"A. Raj, V. P. Namboodiri, T. Tuytelaars, “Mind the Gap: Subspace based Hierarchical Domain Adaptation”, Workshop in Transfer and Multi-View Learning in Advances in Neural Information System Conference (NIPS) 27, Canada, 2014"	2014_task	http://vinaypn.github.io/files/task2014.pdf
2015-05-04	Where is my Friend? - Person identification in Social Networks	Proceedings of the Eleventh IEEE International Conference on Automatic Face and Gesture Recognition (FG 2015)	"One of the interesting applications of computer vision is to be able to identify or detect persons in real world. This problem has been posed in the context of identifying people in television series [2] or in multi-camera networks [8]. However, a common scenario for this problem is to be able to identify people among images prevalent on social networks. In this paper we present a method that aims to solve this problem in real world conditions where the person can be in any pose, profile and orientation and the face itself is not always clearly visible. Moreover, we show that the problem can be solved with as weak supervision only a label whether the person is present or not, which is usually the case as people are tagged in social networks. This is challenging as there can be ambiguity in association of the right person. The problem is solved in this setting using a latent max-margin formulation where the identity of the person is the latent parameter that is classified. This framework builds on other off the shelf computer vision techniques for person detection and face detection and is able to also account for inaccuracies of these components. The idea is to model the complete person in addition to face, that too with weak supervision. We also contribute three real-world datasets that we have created for extensive evaluation of the solution. We show using these datasets that the problem can be effectively solved using the proposed method."	"D. Pathak, Sai Nitish S. and V. P. Namboodiri, “Where is my Friend? - Person identification in Social Networks”, Proceedings of the Eleventh IEEE International Conference on Automatic Face and Gesture Recognition (FG 2015), Ljubljana, Slovenia, 2015"	2015_fg	http://vinaypn.github.io/files/fg2015.pdf
2015-09-07	Subspace alignment based domain adaptation for RCNN detector	Proceedings of British Machine Vision Conference (BMVC)	"In this paper, we propose subspace alignment based domain adaptation of the state of the art RCNN based object detector. The aim is to be able to achieve high quality object detection in novel, real world target scenarios without requiring labels from the target domain. While, unsupervised domain adaptation has been studied in the case of object classification, for object detection it has been relatively unexplored. In subspace based domain adaptation for objects, we need access to source and target subspaces for the bounding box features. The absence of supervision (labels and bounding boxes are absent) makes the task challenging. In this paper, we show that we can still adapt sub- spaces that are localized to the object by obtaining detections from the RCNN detector trained on source and applied on target. Then we form localized subspaces from the detections and show that subspace alignment based adaptation between these subspaces yields improved object detection. This evaluation is done by considering challenging real world datasets of PASCAL VOC as source and validation set of Microsoft COCO dataset as target for various categories."	"Anant Raj, Vinay P. Namboodiri and Tinne Tuytelaars, “Subspace Alignment based Domain Adaptation for RCNN Detector”, Proceedings of British Machine Vision Conference (BMVC 2015), Swansea, UK, 2015"	2015_bmvc_anant	http://vinaypn.github.io/files/bmvc2015rnt.pdf
2015-09-07	Adapting RANSAC SVM to Detect Outliers for Robust Classification.	Proceedings of British Machine Vision Conference (BMVC)	"Most visual classification tasks assume the authenticity of the label information. However, due to several reasons such as difficulty of annotation or inadvertently due to human error, the annotation can often be noisy. This results in examples that are wrongly annotated. In this paper, we consider the examples that are wrongly annotated to be outliers. The task of learning a robust inlier model in the presence of outliers is typically done through the RANSAC algorithm. In this paper, we show that instead of adopting RANSAC to obtain the `right' model, we could use many instances of randomly sampled sets to build lot of models. The collective decision of all these classifiers can be used to identify samples that are likely to be outliers. This results in a modification to RANSAC SVM to explicitly obtain probable outliers from the set of given samples. Once, the outliers are detected, these examples are excluded from the training set. The method can also be used to identify very hard examples from the training set. In this case, where we believe that the examples are correctly annotated, we can achieve good generalization when such examples are excluded from the training set. The method is evaluated using the standard PASCAL VOC dataset. We show that the method is particularly suited for identifying wrongly annotated examples resulting in improvement of more than 12\% over the RANSAC SVM approach. Hard examples in PASCAL VOC dataset are also identified by this method and in fact this even results in a marginal improvement of the classification accuracy over the base classifier provided with all clean samples. "	"Subhabrata Debnath, Anjan Banerjee and Vinay P. Namboodiri, “Adapting RANSAC SVM to detect outliers for Robust Classification”,Proceedings of British Machine Vision Conference (BMVC 2015), Swansea, UK, 2015"	2015_bmvc_subho	http://vinaypn.github.io/files/bmvc2015dbn.pdf
2016-10-09	Deep attributes for one-shot face recognition	 ECCV Workshop on ‘Transfering and Adapting Source Knowledge in Computer Vision’	"We address the problem of one-shot unconstrained face recognition. This is addressed by using a deep attribute representation of faces. While face recognition has considered the use of attribute based representations, for one-shot face recognition, the methods proposed so far have been using different features that represent the limited example available. We postulate that by using an intermediate attribute representation, it is possible to outperform purely face based feature representation for one-shot recognition. We use two one-shot face recognition techniques based on exemplar SVM and one-shot similarity kernel to compare face based deep feature representations against deep attribute based representation. Key ResultThe evaluation on standard dataset of 'Labeled faces in the wild' suggests that deep attribute based representations can outperform deep feature based face representations for this problem of one-shot face recognition."	"A. Jadhav, V.P. Namboodiri and K.S. Venkatesh, “Deep Attributes for One-Shot Face Recog- nition”, ECCV Workshop on ‘Transfering and Adapting Source Knowledge in Computer Vision’, Amsterdam, 2016"	2016_task	http://vinaypn.github.io/files/task2016.pdf
2016-11-20	Using Gaussian Processes to Improve Zero-Shot Learning with Relative Attributes	Proceedings of Asian Conference on Computer Vision (ACCV)	"Relative attributes can serve as a very useful method for zero-shot learning of images. This was shown by the work of Parikh and Grauman [1] where an image is expressed in terms of attributes that are relatively specified between different class pairs. However, for zero-shot learning the authors had assumed a simple Gaussian Mixture Model (GMM) that used the GMM based clustering to obtain the label for an unknown target test example. In this paper, we contribute a principled approach that uses Gaussian Process based classification to obtain the posterior probability for each sample of an unknown target class, in terms of Gaussian process classification and regression for nearest sample images. We analyse different variants of this approach and show that such a principled approach yields improved performance and a better understanding in terms of probabilistic estimates. The method is evaluated on standard Pubfig and Shoes with Attributes benchmarks"	"Y. Dolma and V.P. Namboodiri, “Gaussian Processes to Improve Zero-Shot Learning with Relative Attributes”, Proceedings of Asian Conference in Computer Vision (ACCV), Taipei, Taiwan, 2016"	2016_accv	http://vinaypn.github.io/files/accv2016.pdf
2017-02-04	Contextual rnn-gans for abstract reasoning diagram generation	Thirty-First AAAI Conference on Artificial Intelligence (AAAI)	"Understanding, predicting, and generating object motions and transformations is a core problem in artificial intelligence. Modeling sequences of evolving images may provide better representations and models of motion and may ultimately be used for forecasting, simulation, or video generation. Diagrammatic Abstract Reasoning is an avenue in which diagrams evolve in complex patterns and one needs to infer the underlying pattern sequence and generate the next image in the sequence. For this, we develop a novel Contextual Generative Adversarial Network based on Recurrent Neural Networks (Context-RNN-GANs), where both the generator and the discriminator modules are based on contextual history (modeled as RNNs) and the adversarial discriminator guides the generator to produce realistic images for the particular time step in the image sequence. We evaluate the Context-RNN-GAN model (and its variants) on a novel dataset of Diagrammatic Abstract Reasoning, where it performs competitively with 10th-grade human performance but there is still scope for interesting improvements as compared to college-grade human performance. We also evaluate our model on a standard video next-frame prediction task, achieving improved performance over comparable state-of-the-art."	"A. Ghosh, V. Kulharia, A. Mukerjee, V.P. Namboodiri, M. Bansal, “Contextual RNN-GANs for Abstract Reasoning Diagram Generation”, Proceedings of Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17), San Francisco, California, USA, February 2017"	2017_aaai	http://vinaypn.github.io/files/aaai2017.pdf
2017-02-14	Sketchsoup: Exploratory ideation using design sketches	Computer Graphics Forum (CGF) Journal	"A hallmark of early stage design is a number of quick-and-dirty sketches capturing design inspirations, model variations, and alternate viewpoints of a visual concept. We present SketchSoup, a workflow that allows designers to explore the design space induced by such sketches. We take an unstructured collection of drawings as input, register them using a multi-image matching algorithm, and present them as a 2D interpolation space. By morphing sketches in this space, our approach produces plausible visualizations of shape and viewpoint variations despite the presence of sketch distortions that would prevent standard camera calibration and 3D reconstruction. In addition, our interpolated sketches can serve as inspiration for further drawings, which feed back into the design space as additional image inputs. SketchSoup thus fills a significant gap in the early ideation stage of conceptual design by allowing designers to make better informed choices before proceeding to more expensive 3D modeling and prototyping. From a technical standpoint, we describe an end-to-end system that judiciously combines and adapts various image processing techniques to the drawing domain -- where the images are dominated not by color, shading and texture, but by sketchy stroke contours. "	"R. Arora, I. Darolia, V.P. Namboodiri, K. Singh and A. Bousseau, “SketchSoup: Exploratory Ideation Using Design Sketches”, Computer Graphics Forum, 2017"	2017_cgf	http://vinaypn.github.io/files/cgf2017.pdf
2017-05-16	Compact Environment-Invariant Codes for Robust Visual Place Recognition	14th Conference on Computer and Robot Vision (CRV)	"Robust visual place recognition (VPR) requires scene representations that are invariant to various environmental challenges such as seasonal changes and variations due to ambient lighting conditions during day and night. Moreover, a practical VPR system necessitates compact representations of environmental features. To satisfy these requirements, in this paper we suggest a modification to the existing pipeline of VPR systems to incorporate supervised hashing. The modified system learns (in a supervised setting) compact binary codes from image feature descriptors. These binary codes imbibe robustness to the visual variations exposed to it during the training phase, thereby, making the system adaptive to severe environmental changes. Also, incorporating supervised hashing makes VPR computationally more efficient and easy to implement on simple hardware. This is because binary embeddings can be learned over simple-to-compute features and the distance computation is also in the low-dimensional hamming space of binary codes. We have performed experiments on several challenging data sets covering seasonal, illumination and viewpoint variations. We also compare two widely used supervised hashing methods of CCAITQ and MLH and show that this new pipeline out-performs or closely matches the state-of-the-art deep learning VPR methods that are based on high-dimensional features extracted from pre-trained deep convolutional neural networks. "	"U.Jain, V.P. Namboodiri and G. Pandey,“Supervised Hashing for Robust Visual Place Recognition”, 14th Conference on Computer and Robot Vision, Edmonton, Alberta, May 16-19, 2017"	2017_crv	http://vinaypn.github.io/files/crv2017.pdf
2017-10-09	Reactive Displays for Virtual Reality	IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)	"The feeling of presence in virtual reality has enabled a large number of applications. These applications typically deal with 360° content. However, a large amount of existing content is available in terms of images and videos i.e 2D content. Unfortunately, these do not react to the viewer's position or motion when viewed through a VR HMD. Thus in this work, we propose reactive displays for VR which instigate a feeling of discovery while exploring 2D content. We create this by taking into account user's position and motion to compute homography based mappings that adapt the 2D content and re-project it onto the display. This allows the viewer to obtain a more richer experience of interacting with 2D content similar to the effect of viewing through the window at a scene. We also provide a VR interface that uses a constrained set of reactive displays to easily browse through 360° content. The proposed interface tackles the problem of nausea caused by existing interfaces like photospheres by providing a natural room-like intermediate interface before changing 360° content. We perform user studies to evaluate both of our interfaces. The results show that the proposed reactive display interfaces are indeed beneficial."	"G S S Srinivas Rao, Neeraj Thakur, Vinay P. Namboodiri, “Reactive Displays for Virtual Reality”, Proceedings of 16th IEEE International Symposium on Mixed and Augmented Reality (ISMAR) (Poster Proceedings), Nantes, France, 2017"	2017_ismar	http://vinaypn.github.io/files/ismar2017.pdf
2017-12-16	Visual Odometry Based Omni-directional Hyperlapse	"Proceedings of National Conference on Computer Vision, Pattern Recognition, Image Processing and Graphics (NCVPRIPG 2017),"	"The prohibitive amounts of time required to review the large amounts of data captured by surveillance and other cameras has brought into question the very utility of large scale video logging. Yet, one recognizes that such logging and analysis are indispensable to security applications. The only way out of this paradox is to devise expedited browsing, by the creation of hyperlapse. We address the hyperlapse problem for the very challenging category of intensive egomotion which makes the hyperlapse highly jerky. We propose an economical approach for trajectory estimation based on Visual Odometry and implement cost functions to penalize pose and path deviations. Also, this is implemented on data taken by omni-directional camera, so that the viewer can opt to observe any direction while browsing. This requires many innovations, including handling the massive radial distortions and implementing scene stabilization that need to be operated upon the least distorted region of the omni view"	"P. Rani, A. Jangid, V.P. Namboodiri and K.S. Venkatesh, “Visual Odometry based Omni-directional Hyperlapse”, Proceedings of National Conference on Computer Vision, Pattern Recognition, Image Processing and Graphics (NCVPRIPG 2017), Mandi, India 2017"	2017_ncvpripg	http://vinaypn.github.io/files/ncvpripg2017.pdf
