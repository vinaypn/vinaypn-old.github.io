pub_date	title	venue	excerpt	citation	url_slug	paper_url
2004-10-01	Image retrieval based on projective invariance	IEEE International Conference on Image Processing (ICIP)	"We propose an image retrieval scheme based on projectively invariant features. Since cross-ratio is the fundamental invariant feature under projective transformations for points, we use that as the basic feature parameter. We compute the cross-ratios of point sets in quadruplets and a discrete representation of the distribution of the cross-ratio is obtained from the computed values. The distribution is used as the feature for retrieval purposes. The method is very effective in retrieving images, like buildings, having similar planar 3D structures."	"Rajashekhar, S. Chaudhuri and V. P. Namboodiri (2004). ""Image retrieval based on projective invariance."" <i>IEEE International Conference on Image Processing</i> Singapore, October 2004, Page 405-408"	2004_icip	https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1418776
2004-12-16	Use of Linear Diffusion in depth estimation based on defocus cue	"Fourth Indian Conference on Computer Vision, Graphics & Image Processing"	"Diffusion has been used extensively in computer vision. Most common applications of diffusion have been in low level vision problems like segmentation and edge detection. In this paper a novel application of the linear diffusion principle is made for the estimation of depth using the properties of the real aperture imaging system. The method uses two defocused images of a scene and the lens parameter setting as input and estimates the depth in the scene, and also generates the corresponding fully focused equivalent pin-hole image. The algorithm described here also brings out the equivalence of the two modalities, viz. depth from focus and depth from defocus for structure recovery."	"V.P. Namboodiri and S. Chaudhuri (2004). “Use of Linear Diffusion in depth estimation based on defocus cue” <i> Proceedings of Fourth Indian Conference on Computer Vision, Graphics & Image Processing (ICVGIP) </i> Kolkata, India. December 2004."	2004_icvgip	http://vinaypn.github.io/files/icvgip04.pdf
2005-06-20	Shock Filters based on Implicit Cluster Separation	IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)	One of the classic problems in low level vision is image restoration. An important contribution toward this effort has been the development of shock filters by Osher and Rudin (1990). It performs image deblurring using hyperbolic partial differential equations. In this paper we relate the notion of cluster separation from the field of pattern recognition to the shock filter formulation. A kind of shock filter is proposed based on the idea of gradient based separation of clusters. The proposed formulation is general enough as it can allow various models of density functions in the cluster separation process. The efficacy of the method is demonstrated through various examples.	"V.P. Namboodiri and S. Chaudhuri  (2005). ""Shock Filters based on Implicit Cluster Separation."" <i>Proc. of IEEE International Conference on Computer Vision and Pattern Recognition (CVPR),</i>San Diego June 2005, Page 1-6."	2005_cvpr	http://vinaypn.github.io/files/cvpr05.pdf
2006-12-13	Improved Kernel-Based Object Tracking Under Occluded Scenarios	"Fifth Indian Conference on Computer Vision, Graphics & Image Processing"	"A successful approach for object tracking has been kernel based object tracking [1] by Comaniciu et al.. The method provides an effective solution to the problems of representation and localization in tracking. The method involves representation of an object by a feature histogram with an isotropic kernel and performing a gradient based mean shift optimization for localizing the kernel. Though robust, this technique fails under cases of occlusion. We improve the kernel based object tracking by performing the localization using a generalized (bidirectional) mean shift based optimization. This makes the method resilient to occlusions. Another aspect related to the localization step is handling of scale changes by varying the bandwidth of the kernel. Here, we suggest a technique based on SIFT features [2] by Lowe to enable change of bandwidth of the kernel even in the presence of occlusion. We demonstrate the effectiveness of the techniques proposed through extensive experimentation on a number of challenging data sets."	"V.P. Namboodiri, A. Ghorawat and S. Chaudhuri (2006) “Improved Kernel-Based Object Tracking Under Occluded Scenarios”.<i> In: Kalra P.K., Peleg S. (eds) Computer Vision, Graphics and Image Processing. Lecture Notes in Computer Science, vol 4338.</i> Springer, Berlin, Heidelberg"	2006_icvgip	http://vinaypn.github.io/files/icvgip06.pdf
2007-01-01	Retrieval of images of man-made structures based on projective invariance	Pattern Recognition Journal	"In this paper we propose a geometry-based image retrieval scheme that makes use of projectively invariant features. Cross-ratio (CR) is an invariant feature under projective transformations for collinear points. We compute the CRs of point sets in quadruplets and the CR histogram is used as the feature for retrieval purposes. Being a geometric feature, it allows us to retrieve similar images irrespective of view point and illumination changes. We can retrieve the same building even if the facade has undergone a fresh coat of paints! Color and textural features can also be included, if desired. Experimental results show a favorably very good retrieval accuracy when tested on an image database of size 4000. The method is very effective in retrieving images having man-made objects rich in polygonal structures like buildings, rail tracks, etc."	"Rajashekhar, S. Chaudhuri and V.P. Namboodiri (2007), “Retrieval of images of man-made structures based on projective invariance”. <i>Pattern Recognition Journal, Volume 40, Issue 1, January 2007</i>, Pages 296-308"	2007_pr	https://www.sciencedirect.com/science/article/pii/S0031320306001671
2007-02-01	"On defocus, diffusion and depth estimation"	Pattern Recognition Letters	"An intrinsic property of real aperture imaging has been that the observations tend to be defocused. This artifact has been used in an innovative manner by researchers for depth estimation, since the amount of defocus varies with varying depth in the scene. There have been various methods to model the defocus blur. We model the defocus process using the model of diffusion of heat. The diffusion process has been traditionally used in low level vision problems like smoothing, segmentation and edge detection. In this paper a novel application of the diffusion principle is made for generating the defocus space of the scene. The defocus space is the set of all possible observations for a given scene that can be captured using a physical lens system. Using the notion of defocus space we estimate the depth in the scene and also generate the corresponding fully focused equivalent pin-hole image. The algorithm described here also brings out the equivalence of the two modalities, viz. depth from focus and depth from defocus for structure recovery."	"V.P. Namboodiri and S. Chaudhuri  (2007). “On defocus, diffusion and depth estimation” <i>Pattern Recognition Letters</i> Volume 28, Issue 3, 1 February 2007, Pages 311-319"	2007_prl	http://vinaypn.github.io/files/prl07.pdf
2007-05-30	Super-Resolution Using Sub-band Constrained Total Variation	International Conference on Scale Space and Variational Methods in Computer Vision (SSVM)	"Super-resolution of a single image is a severely ill-posed problem in computer vision. It is possible to consider solving this problem by considering a total variation based regularization framework. The choice of total variation based regularization helps in formulating an edge preserving scheme for super-resolution. However, this scheme tends to result in a piece-wise constant resultant image. To address this issue, we extend the formulation by incorporating an appropriate sub-band constraint which ensures the preservation of textural details in trade off with noise present in the observation. The proposed framework is extensively evaluated and the experimental results for the same are presented"	"P. Chatterjee,  V.P. Namboodiri, S. Chaudhuri (2007) “Super-Resolution Using Sub-band Constrained Total Variation” <i> In: Sgallari F., Murli A., Paragios N. (eds) Scale Space and Variational Methods in Computer Vision SSVM 2007.</i> Lecture Notes in Computer Science, vol 4485. Springer, Berlin, Heidelberg"	2007_ssvm	http://vinaypn.github.io/files/ssvm07.pdf
2007-09-10	Shape Recovery Using Stochastic Heat Flow	British Machine Vision Conference (BMVC)	"We consider the problem of depth estimation from multiple images based on the defocus cue. For a Gaussian defocus blur, the observations can be shown to be the solution of a deterministic but inhomogeneous diffusion process. However, the diffusion process does not sufficiently address the case in which the Gaussian kernel is deformed. This deformation happens due to several factors like self-occlusion, possible aberrations and imperfections in the aperture. These issues can be solved by incorporating a stochastic perturbation into the heat diffusion process. The resultant flow is that of an inhomogeneous heat diffusion perturbed by a stochastic curvature driven motion. The depth in the scene is estimated from the coefficient of the stochastic heat equation without actually knowing the departure from the Gaussian assumption. Further, the proposed method also takes into account the non-convex nature of the diffusion process. The method provides a strong theoretical framework for handling the depth from defocus problem."	"V.P. Namboodiri and S. Chaudhuri  (2007) “Shape Recovery Using Stochastic Heat Flow“<i> Proceedings of the British Machine Vision Conference 2007</i>, University of Warwick, UK, September 10-13, 2007"	2007_bmvc	http://vinaypn.github.io/files/bmvc07.pdf
2007-10-19	Image Restoration using Geometrically Stabilized Reverse Heat Equation	IEEE International Conference on Image Processing (ICIP)	"Blind restoration of blurred images is a classical ill-posed problem. There has been considerable interest in the use of partial differential equations to solve this problem. The blurring of an image has traditionally been modeled by Witkin [10] and Koenderink [4] by the heat equation. This has been the basis of the Gaussian scale space. However, a similar theoretical formulation has not been possible for deblurring of images due to the ill-posed nature of the reverse heat equation. Here we consider the stabilization of the reverse heat equation. We do this by damping the distortion along the edges by adding a normal component of the heat equation in the forward direction. We use a stopping criterion based on the divergence of the curvature in the resulting reverse heat flow. The resulting stabilized reverse heat flow makes it possible to solve the challenging problem of blind space varying deconvolution. The method is justified by a varied set of experimental results."	"V.P. Namboodiri and S. Chaudhuri  (2005). ""Image Restoration using Geometrically Stabilized Reverse Heat Equation."" <i>Proceedings of IEEE International Conference on Image Processing (ICIP)</i>, San Antonio, Texas, USA, 2007, Pages IV - 413 - 416."	2007_icip	http://vinaypn.github.io/files/icip07.pdf
2008-06-23	Recovery of relative depth from a single observation using an uncalibrated (real-aperture) camera	IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	"In this paper we investigate the challenging problem of recovering the depth layers in a scene from a single defocused observation. The problem is definitely solvable if there are multiple observations. In this paper we show that one can perceive the depth in the scene even from a single observation. We use the inhomogeneous reverse heat equation to obtain an estimate of the blur, thereby preserving the depth information characterized by the defocus. However, the reverse heat equation, due to its parabolic nature, is divergent. We stabilize the reverse heat equation by considering the gradient degeneration as an effective stopping criterion. The amount of (inverse) diffusion is actually a measure of relative depth. Because of ill-posedness we propose a graph-cuts based method for inferring the depth in the scene using the amount of diffusion as a data likelihood and a smoothness condition on the depth in the scene. The method is verified experimentally on a varied set of test cases."	"V.P. Namboodiri and S. Chaudhuri  (2008). ""Recovery of relative depth from a single observation using an uncalibrated (real-aperture) camera."" <i>Proc. of IEEE International Conference on Computer Vision and Pattern Recognition (CVPR),</i>Anchorage, AK, USA, June 2008, Page 1-6."	2008_cvpr	http://vinaypn.github.io/files/cvpr08.pdf
2008-10-12	Regularized depth from defocus	IEEE International Conference on Image Processing (ICIP)	"n the area of depth estimation from images an interesting approach has been structure recovery from defocus cue. Towards this end, there have been a number of approaches [4,6]. Here we propose a technique to estimate the regularized depth from defocus using diffusion. The coefficient of the diffusion equation is modeled using a pair-wise Markov random field (MRF) ensuring spatial regularization to enhance the robustness of the depth estimated. This framework is solved efficiently using a graph-cuts based techniques. The MRF representation is enhanced by incorporating a smoothness prior that is obtained from a graph based segmentation of the input images. The method is demonstrated on a number of data sets and its performance is compared with state of the art techniques."	"V.P. Namboodiri, S. Chaudhuri and S. Hadap (2008). “Regularized depth from defocus”, <i> Proceedings of IEEE International Conference on Image Processing  (ICIP) </i>, San Diego, CA, USA, pp. 1520-1523."	2008_icip	http://vinaypn.github.io/files/icip08.pdf
2011-01-05	Action Recognition: A Region Based Approach	 IEEE Workshop on Applications of Computer Vision (WACV)	"We address the problem of recognizing actions in reallife videos. Space-time interest point-based approaches have been widely prevalent towards solving this problem. In contrast, more spatially extended features such as regions have not been so popular. The reason is, any local region based approach requires the motion flow information for a specific region to be collated temporally. This is challenging as the local regions are deformable and not well delineated from the surroundings. In this paper we address this issue by using robust tracking of regions and we show that it is possible to obtain region descriptors for classification of actions. This paper lays the groundwork for further investigation into region based approaches. Through this paper we make the following contributions a) We advocate identification of salient regions based on motion segmentation b) We adopt a state-of-the art tracker for robust tracking of the identified regions rather than using isolated space-time blocks c) We propose optical flow based region descriptors to encode the extracted trajectories in piece-wise blocks. We demonstrate the performance of our system on real-world data sets."	"H. Bilen, V.P. Namboodiri and L. Van Gool (2011). “Action recognition: A region based approach”, <i> 2011 IEEE Workshop on Applications of Computer Vision (WACV), Kona, HI</i> , 2011, pp. 294-300"	2011_wacv	http://vinaypn.github.io/files/wacv2011.pdf
2011-09-02	Object and Action Classification with Latent Variables	British Machine Vision Conference	In this paper we propose a generic framework to incorporate unobserved auxiliary information for classifying objects and actions. This framework allows us to explicitly account for localisation and alignment of representations for generic object and action classes as latent variables. We approach this problem in the discriminative setting as learning a max-margin classifier that infers the class label along with the latent variables. Through this paper we make the following contributions a) We provide a method for incorporating latent variables into object and action classification b) We specifically account for the presence of an explicit class related subregion which can include foreground and/or background. c) We explore a way to learn a better classifier by iterative expansion of the latent parameter space. We demonstrate the performance of our approach by rigorous experimental evaluation on a number of standard object and action recognition datasets. <br> <b> Awarded: Best Paper Prize </b>	"H. Bilen, V.P. Namboodiri and L. Van Gool (2011). “Object and Action Classification with Latent Variables”, <i> In Jesse Hoey, Stephen McKenna and Emanuele Trucco, Proceedings of the British Machine Vision Conference</i>, pages 17.1-17.11. BMVA Press, September 2011"	2011_bmvc	http://vinaypn.github.io/files/bmvc2011.pdf
2011-09-22	Super-resolution techniques for minimally invasive surgery	MICCAI workshop on augmented environments for computer assisted interventions-AE-CAI	"We propose the use of super-resolution techniques to aid visualization while carrying out minimally invasive surgical procedures. These procedures are performed using small endoscopic cameras, which inherently have limited imaging resolution. The use of higher-end cam- eras is technologically challenging and currently not yet cost effective. A promising alternative is to consider improving the resolution by post- processing the acquired images through the use of currently prevalent super-resolution techniques. In this paper we analyse the different method- ologies that have been proposed for super-resolution and provide a comprehensive evaluation of the most significant algorithms. The methods are evaluated using challenging in-vivo real world medical datasets. We suggest that the use of a learning-based super-resolution algorithm com- bined with an edge-directed approach would be most suited for this application. "	"V. De Smet, V.P. Namboodiri and L. Van Gool (2011). “Super-resolution techniques for minimally invasive surgery”, <i> 6th MICCAI workshop on augmented environments for computer assisted interventions-AE-CAI 2011 </i>, Toronto,Canada."	2011_aecai	http://vinaypn.github.io/files/AECAI.pdf
2011-11-06	Systematic evaluation of super-resolution using classification	 Visual Communications and Image Processing (VCIP)	"Currently two evaluation methods of super-resolution (SR) techniques prevail: The objective Peak Signal to Noise Ratio (PSNR) and a qualitative measure based on manual visual inspection. Both of these methods are sub-optimal: The latter does not scale well to large numbers of images, while the former does not necessarily reflect the perceived visual quality. We address these issues in this paper and propose an evaluation method based on image classification. We show that perceptual image quality measures like structural similarity are not suitable for evaluation of SR methods. On the other hand a systematic evaluation using large datasets of thousands of real-world images provides a consistent comparison of SR algorithms that corresponds to perceived visual quality. We verify the success of our approach by presenting an evaluation of three recent super-resolution algorithms on standard image classification datasets."	"V. De Smet, V.P. Namboodiri and L. Van Gool (2011). “Systematic evaluation of super-resolution using classification”, <i> 2011 Visual Communications and Image Processing (VCIP), Tainan</i>, 2011, pp. 1-4."	2011_vcip	http://vinaypn.github.io/files/VCIP.pdf
2012-08-28	"Classification with Global, Local and Shared Features"	Joint DAGM (German Association for Pattern Recognition) and OAGM Symposium	"We present a framework that jointly learns and then uses multiple image windows for improved classification. Apart from using the entire image content as context, class-specific windows are added, as well as windows that target class pairs. The location and extent of the windows are set automatically by handling the window parameters as latent variables. This framework makes the following contributions: a) the addition of localized information through the class-specific windows improves classification, b) windows introduced for the classification of class pairs further improve the results, c) the windows and classification parameters can be effectively learnt using a discriminative max-margin approach with latent variables, and d) the same framework is suited for multiple visual tasks such as classifying objects, scenes and actions. Experiments demonstrate the aforementioned claims."	"Bilen H., Namboodiri V.P., Van Gool L.J. (2012) Classification with Global, Local and Shared Features. In: Pinz A., Pock T., Bischof H., Leberl F. (eds) <i>Joint DAGM (German Association for Pattern Recognition) and OAGM Symposium.</i> Lecture Notes in Computer Science, vol 7476. Springer, Berlin, Heidelberg, pp 134-143"	2012_dagm	http://vinaypn.github.io/files/dagm2012.pdf
2013-01-15	Nonuniform Image Patch Exemplars for Low Level Vision	IEEE Workshop on Applications of Computer Vision (WACV)	"We approach the classification problem in a discrim- inative setting, as learning a max-margin classifier that infers the class label along with the latent variables. Through this paper we make the following contribu- tions: a) we provide a method for incorporating latent variables into object and action classification; b) these variables determine the relative focus on foreground vs. background information that is taken account of; c) we design an objective function to more effectively learn in unbalanced data sets; d) we learn a better classifier by iterative expansion of the latent parameter space. We demonstrate the performance of our approach through"	"V. De Smet, L. Van Gool and V. P. Namboodiri, ""Nonuniform image patch exemplars for low level vision,"" <i>2013 IEEE Workshop on Applications of Computer Vision (WACV)</i>, Tampa, FL, 2013, pp. 23-30."	2013_wacv	http://vinaypn.github.io/files/wacv2013.pdf
2014-02-01	Object and Action Classification with Latent Window Parameters	International Journal of Computer Vision (IJCV)	"In this paper we propose a generic framework to incorporate unobserved auxiliary information for classifying objects and actions. This framework allows us to automatically select a bounding box and its quadrants from which best to extract features. These spatial subdivisions are learnt as latent variables. The paper is an extended version of our earlier work [2], complemented with additional ideas, experiments and analysis. <br> We approach the classification problem in a discriminative setting, as learning a max-margin classifier that infers the class label along with the latent variables. Through this paper we make the following contributions: a) we provide a method for incorporating latent variables into object and action classification; b) these variables determine the relative focus on foreground vs. background information that is taken account of; c) we design an objective function to more effectively learn in unbalanced data sets; d) we learn a better classifier by iterative expansion of the latent parameter space. We demonstrate the performance of our approach through experimental evaluation on a number of standard object and action recognition data sets."	"H. Bilen, V.P. Namboodiri and L. Van Gool (2014), “Object and Action Classification with Latent Window Parameters”, <i>International Journal of Computer Vision (IJCV)</i> Vol: 106: 237 - 251, February 2014"	2014_ijcv	http://vinaypn.github.io/files/ijcv2014.pdf
2014-06-23	Object classification with adaptable regions	IEEE Conference on Computer Vision and Pattern Recognition	"In classification of objects substantial work has gone into improving the low level representation of an image by considering various aspects such as different features, a number of feature pooling and coding techniques and considering different kernels. Unlike these works, in this paper, we propose to enhance the semantic representation of an image. We aim to learn the most important visual components of an image and how they interact in order to classify the objects correctly. To achieve our objective, we propose a new latent SVM model for category level object classification. Starting from image-level annotations, we jointly learn the object class and its context in terms of spatial location (where) and appearance (what). Furthermore, to regularize the complexity of the model we learn the spatial and co-occurrence relations between adjacent regions, such that unlikely configurations are penalized. Experimental results demonstrate that the proposed method can consistently enhance results on the challenging Pascal VOC dataset in terms of classification and weakly supervised detection. We also show how semantic representation can be exploited for finding similar content."	"H. Bilen, M. Pedersoli, V. P. Namboodiri, T. Tuytelaars, L. Van Gool,“Object Classification with Adaptable Regions”, Proceedings of IEEE International Conference on Computer Vision and Pattern Recognition (CVPR), 2014"	2014_cvpr	http://vinaypn.github.io/files/cvpr2014.pdf
2014-12-13	Mind the gap: Subspace based hierarchical domain adaptation	"Workshop in Transfer and Multi-View Learning in Advances in Neural Information System Conference (NIPS) 27,"	"Domain adaptation techniques aim at adapting a classifier learnt on a source domain to work on the target domain. Exploiting the subspaces spanned by features of the source and target domains respectively is one approach that has been investigated towards solving this problem. These techniques normally assume the existence of a single subspace for the entire source / target domain. In this work, we consider the hierarchical organization of the data and consider multiple subspaces for the source and target domain based on the hierarchy. We evaluate different subspace based domain adaptation techniques under this setting and observe that using different subspaces based on the hierarchy yields consistent improvement over a non-hierarchical baseline"	"A. Raj, V. P. Namboodiri, T. Tuytelaars, “Mind the Gap: Subspace based Hierarchical Domain Adaptation”, Workshop in Transfer and Multi-View Learning in Advances in Neural Information System Conference (NIPS) 27, Canada, 2014"	2014_task	http://vinaypn.github.io/files/task2014.pdf
2015-05-04	Where is my Friend? - Person identification in Social Networks	Proceedings of the Eleventh IEEE International Conference on Automatic Face and Gesture Recognition (FG 2015)	"One of the interesting applications of computer vision is to be able to identify or detect persons in real world. This problem has been posed in the context of identifying people in television series [2] or in multi-camera networks [8]. However, a common scenario for this problem is to be able to identify people among images prevalent on social networks. In this paper we present a method that aims to solve this problem in real world conditions where the person can be in any pose, profile and orientation and the face itself is not always clearly visible. Moreover, we show that the problem can be solved with as weak supervision only a label whether the person is present or not, which is usually the case as people are tagged in social networks. This is challenging as there can be ambiguity in association of the right person. The problem is solved in this setting using a latent max-margin formulation where the identity of the person is the latent parameter that is classified. This framework builds on other off the shelf computer vision techniques for person detection and face detection and is able to also account for inaccuracies of these components. The idea is to model the complete person in addition to face, that too with weak supervision. We also contribute three real-world datasets that we have created for extensive evaluation of the solution. We show using these datasets that the problem can be effectively solved using the proposed method."	"D. Pathak, Sai Nitish S. and V. P. Namboodiri, “Where is my Friend? - Person identification in Social Networks”, Proceedings of the Eleventh IEEE International Conference on Automatic Face and Gesture Recognition (FG 2015), Ljubljana, Slovenia, 2015"	2015_fg	http://vinaypn.github.io/files/fg2015.pdf
2015-09-07	Subspace alignment based domain adaptation for RCNN detector	Proceedings of British Machine Vision Conference (BMVC)	"In this paper, we propose subspace alignment based domain adaptation of the state of the art RCNN based object detector. The aim is to be able to achieve high quality object detection in novel, real world target scenarios without requiring labels from the target domain. While, unsupervised domain adaptation has been studied in the case of object classification, for object detection it has been relatively unexplored. In subspace based domain adaptation for objects, we need access to source and target subspaces for the bounding box features. The absence of supervision (labels and bounding boxes are absent) makes the task challenging. In this paper, we show that we can still adapt sub- spaces that are localized to the object by obtaining detections from the RCNN detector trained on source and applied on target. Then we form localized subspaces from the detections and show that subspace alignment based adaptation between these subspaces yields improved object detection. This evaluation is done by considering challenging real world datasets of PASCAL VOC as source and validation set of Microsoft COCO dataset as target for various categories."	"Anant Raj, Vinay P. Namboodiri and Tinne Tuytelaars, “Subspace Alignment based Domain Adaptation for RCNN Detector”, Proceedings of British Machine Vision Conference (BMVC 2015), Swansea, UK, 2015"	2015_bmvc_anant	http://vinaypn.github.io/files/bmvc2015rnt.pdf
2015-09-07	Adapting RANSAC SVM to Detect Outliers for Robust Classification.	Proceedings of British Machine Vision Conference (BMVC)	"Most visual classification tasks assume the authenticity of the label information. However, due to several reasons such as difficulty of annotation or inadvertently due to human error, the annotation can often be noisy. This results in examples that are wrongly annotated. In this paper, we consider the examples that are wrongly annotated to be outliers. The task of learning a robust inlier model in the presence of outliers is typically done through the RANSAC algorithm. In this paper, we show that instead of adopting RANSAC to obtain the `right' model, we could use many instances of randomly sampled sets to build lot of models. The collective decision of all these classifiers can be used to identify samples that are likely to be outliers. This results in a modification to RANSAC SVM to explicitly obtain probable outliers from the set of given samples. Once, the outliers are detected, these examples are excluded from the training set. The method can also be used to identify very hard examples from the training set. In this case, where we believe that the examples are correctly annotated, we can achieve good generalization when such examples are excluded from the training set. The method is evaluated using the standard PASCAL VOC dataset. We show that the method is particularly suited for identifying wrongly annotated examples resulting in improvement of more than 12\% over the RANSAC SVM approach. Hard examples in PASCAL VOC dataset are also identified by this method and in fact this even results in a marginal improvement of the classification accuracy over the base classifier provided with all clean samples."	"Subhabrata Debnath, Anjan Banerjee and Vinay P. Namboodiri, “Adapting RANSAC SVM to detect outliers for Robust Classification”,Proceedings of British Machine Vision Conference (BMVC 2015), Swansea, UK, 2015"	2015_bmvc_subho	http://vinaypn.github.io/files/bmvc2015dbn.pdf
2016-10-09	Deep attributes for one-shot face recognition	 ECCV Workshop on ‘Transfering and Adapting Source Knowledge in Computer Vision’	"We address the problem of one-shot unconstrained face recognition. This is addressed by using a deep attribute representation of faces. While face recognition has considered the use of attribute based representations, for one-shot face recognition, the methods proposed so far have been using different features that represent the limited example available. We postulate that by using an intermediate attribute representation, it is possible to outperform purely face based feature representation for one-shot recognition. We use two one-shot face recognition techniques based on exemplar SVM and one-shot similarity kernel to compare face based deep feature representations against deep attribute based representation. Key ResultThe evaluation on standard dataset of 'Labeled faces in the wild' suggests that deep attribute based representations can outperform deep feature based face representations for this problem of one-shot face recognition."	"A. Jadhav, V.P. Namboodiri and K.S. Venkatesh, “Deep Attributes for One-Shot Face Recog- nition”, ECCV Workshop on ‘Transfering and Adapting Source Knowledge in Computer Vision’, Amsterdam, 2016"	2016_task	http://vinaypn.github.io/files/task2016.pdf
2016-11-20	Using Gaussian Processes to Improve Zero-Shot Learning with Relative Attributes	Proceedings of Asian Conference on Computer Vision (ACCV)	"Relative attributes can serve as a very useful method for zero-shot learning of images. This was shown by the work of Parikh and Grauman [1] where an image is expressed in terms of attributes that are relatively specified between different class pairs. However, for zero-shot learning the authors had assumed a simple Gaussian Mixture Model (GMM) that used the GMM based clustering to obtain the label for an unknown target test example. In this paper, we contribute a principled approach that uses Gaussian Process based classification to obtain the posterior probability for each sample of an unknown target class, in terms of Gaussian process classification and regression for nearest sample images. We analyse different variants of this approach and show that such a principled approach yields improved performance and a better understanding in terms of probabilistic estimates. The method is evaluated on standard Pubfig and Shoes with Attributes benchmarks"	"Y. Dolma and V.P. Namboodiri, “Gaussian Processes to Improve Zero-Shot Learning with Relative Attributes”, Proceedings of Asian Conference in Computer Vision (ACCV), Taipei, Taiwan, 2016"	2016_accv	http://vinaypn.github.io/files/accv2016.pdf
2017-02-04	Contextual rnn-gans for abstract reasoning diagram generation	Thirty-First AAAI Conference on Artificial Intelligence (AAAI)	"Understanding, predicting, and generating object motions and transformations is a core problem in artificial intelligence. Modeling sequences of evolving images may provide better representations and models of motion and may ultimately be used for forecasting, simulation, or video generation. Diagrammatic Abstract Reasoning is an avenue in which diagrams evolve in complex patterns and one needs to infer the underlying pattern sequence and generate the next image in the sequence. For this, we develop a novel Contextual Generative Adversarial Network based on Recurrent Neural Networks (Context-RNN-GANs), where both the generator and the discriminator modules are based on contextual history (modeled as RNNs) and the adversarial discriminator guides the generator to produce realistic images for the particular time step in the image sequence. We evaluate the Context-RNN-GAN model (and its variants) on a novel dataset of Diagrammatic Abstract Reasoning, where it performs competitively with 10th-grade human performance but there is still scope for interesting improvements as compared to college-grade human performance. We also evaluate our model on a standard video next-frame prediction task, achieving improved performance over comparable state-of-the-art."	"A. Ghosh, V. Kulharia, A. Mukerjee, V.P. Namboodiri, M. Bansal, “Contextual RNN-GANs for Abstract Reasoning Diagram Generation”, Proceedings of Thirty-First AAAI Conference on Artificial Intelligence (AAAI-17), San Francisco, California, USA, February 2017"	2017_aaai	http://vinaypn.github.io/files/aaai2017.pdf
2017-02-14	Sketchsoup: Exploratory ideation using design sketches	Computer Graphics Forum (CGF) Journal	"A hallmark of early stage design is a number of quick-and-dirty sketches capturing design inspirations, model variations, and alternate viewpoints of a visual concept. We present SketchSoup, a workflow that allows designers to explore the design space induced by such sketches. We take an unstructured collection of drawings as input, register them using a multi-image matching algorithm, and present them as a 2D interpolation space. By morphing sketches in this space, our approach produces plausible visualizations of shape and viewpoint variations despite the presence of sketch distortions that would prevent standard camera calibration and 3D reconstruction. In addition, our interpolated sketches can serve as inspiration for further drawings, which feed back into the design space as additional image inputs. SketchSoup thus fills a significant gap in the early ideation stage of conceptual design by allowing designers to make better informed choices before proceeding to more expensive 3D modeling and prototyping. From a technical standpoint, we describe an end-to-end system that judiciously combines and adapts various image processing techniques to the drawing domain -- where the images are dominated not by color, shading and texture, but by sketchy stroke contours."	"R. Arora, I. Darolia, V.P. Namboodiri, K. Singh and A. Bousseau, “SketchSoup: Exploratory Ideation Using Design Sketches”, Computer Graphics Forum, 2017"	2017_cgf	http://vinaypn.github.io/files/cgf2017.pdf
2017-05-16	Compact Environment-Invariant Codes for Robust Visual Place Recognition	14th Conference on Computer and Robot Vision (CRV)	"Robust visual place recognition (VPR) requires scene representations that are invariant to various environmental challenges such as seasonal changes and variations due to ambient lighting conditions during day and night. Moreover, a practical VPR system necessitates compact representations of environmental features. To satisfy these requirements, in this paper we suggest a modification to the existing pipeline of VPR systems to incorporate supervised hashing. The modified system learns (in a supervised setting) compact binary codes from image feature descriptors. These binary codes imbibe robustness to the visual variations exposed to it during the training phase, thereby, making the system adaptive to severe environmental changes. Also, incorporating supervised hashing makes VPR computationally more efficient and easy to implement on simple hardware. This is because binary embeddings can be learned over simple-to-compute features and the distance computation is also in the low-dimensional hamming space of binary codes. We have performed experiments on several challenging data sets covering seasonal, illumination and viewpoint variations. We also compare two widely used supervised hashing methods of CCAITQ and MLH and show that this new pipeline out-performs or closely matches the state-of-the-art deep learning VPR methods that are based on high-dimensional features extracted from pre-trained deep convolutional neural networks. "	"U.Jain, V.P. Namboodiri and G. Pandey,“Supervised Hashing for Robust Visual Place Recognition”, 14th Conference on Computer and Robot Vision, Edmonton, Alberta, May 16-19, 2017"	2017_crv	http://vinaypn.github.io/files/crv2017.pdf
2017-10-09	Reactive Displays for Virtual Reality	IEEE International Symposium on Mixed and Augmented Reality (ISMAR-Adjunct)	"The feeling of presence in virtual reality has enabled a large number of applications. These applications typically deal with 360° content. However, a large amount of existing content is available in terms of images and videos i.e 2D content. Unfortunately, these do not react to the viewer's position or motion when viewed through a VR HMD. Thus in this work, we propose reactive displays for VR which instigate a feeling of discovery while exploring 2D content. We create this by taking into account user's position and motion to compute homography based mappings that adapt the 2D content and re-project it onto the display. This allows the viewer to obtain a more richer experience of interacting with 2D content similar to the effect of viewing through the window at a scene. We also provide a VR interface that uses a constrained set of reactive displays to easily browse through 360° content. The proposed interface tackles the problem of nausea caused by existing interfaces like photospheres by providing a natural room-like intermediate interface before changing 360° content. We perform user studies to evaluate both of our interfaces. The results show that the proposed reactive display interfaces are indeed beneficial."	"G S S Srinivas Rao, Neeraj Thakur, Vinay P. Namboodiri, “Reactive Displays for Virtual Reality”, Proceedings of 16th IEEE International Symposium on Mixed and Augmented Reality (ISMAR) (Poster Proceedings), Nantes, France, 2017"	2017_ismar	http://vinaypn.github.io/files/ismar2017.pdf
2017-12-16	Visual Odometry Based Omni-directional Hyperlapse	"Proceedings of National Conference on Computer Vision, Pattern Recognition, Image Processing and Graphics (NCVPRIPG 2017),"	"The prohibitive amounts of time required to review the large amounts of data captured by surveillance and other cameras has brought into question the very utility of large scale video logging. Yet, one recognizes that such logging and analysis are indispensable to security applications. The only way out of this paradox is to devise expedited browsing, by the creation of hyperlapse. We address the hyperlapse problem for the very challenging category of intensive egomotion which makes the hyperlapse highly jerky. We propose an economical approach for trajectory estimation based on Visual Odometry and implement cost functions to penalize pose and path deviations. Also, this is implemented on data taken by omni-directional camera, so that the viewer can opt to observe any direction while browsing. This requires many innovations, including handling the massive radial distortions and implementing scene stabilization that need to be operated upon the least distorted region of the omni view"	"P. Rani, A. Jangid, V.P. Namboodiri and K.S. Venkatesh, “Visual Odometry based Omni-directional Hyperlapse”, Proceedings of National Conference on Computer Vision, Pattern Recognition, Image Processing and Graphics (NCVPRIPG 2017), Mandi, India 2017"	2017_ncvpripg	http://vinaypn.github.io/files/ncvpripg2017.pdf
2018-06-18	Multi-agent diverse generative adversarial networks	IEEE Conference on Computer Vision and Pattern Recognition	"We propose MAD-GAN, an intuitive generalization to the Generative Adversarial Networks (GANs) and its conditional variants to address the well known problem of mode collapse. First, MAD-GAN is a multi-agent GAN architecture incorporating multiple generators and one discriminator. Second, to enforce that different generators capture diverse high probability modes, the discriminator of MAD-GAN is designed such that along with finding the real and fake samples, it is also required to identify the generator that generated the given fake sample. Intuitively, to succeed in this task, the discriminator must learn to push different generators towards different identifiable modes. We perform extensive experiments on synthetic and real datasets and compare MAD-GAN with different variants of GAN. We show high quality diverse sample generations for challenging tasks such as image-to-image translation and face generation. In addition, we also show that MAD-GAN is able to disentangle different modalities when trained using highly challenging diverse-class dataset (e.g. dataset with images of forests, icebergs, and bedrooms). In the end, we show its efficacy on the unsupervised feature representation task."	"A. Ghosh, V. Kulharia, V.P. Namboodiri, P.H.S. Torr and P. Dokania, “Multi-Agent Diverse Generative Adversarial Networks”, <i>Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>Salt Lake City, Utah, June 2018."	2018_cvpr_arnab_viveka	http://openaccess.thecvf.com/content_cvpr_2018/html/Ghosh_Multi-Agent_Diverse_Generative_CVPR_2018_paper.html
2018-02-02	No Modes left behind: Capturing the data distribution effectively using GANs	Thirty-Second AAAI Conference on Artificial Intelligence (AAAI)	"Generative adversarial networks (GANs) while being very versatile in realistic image synthesis, still are sensitive to the input distribution. Given a set of data that has an imbalance in the distribution, the networks are susceptible to missing modes and not capturing the data distribution. While various methods have been tried to improve training of GANs, these have not addressed the challenges of covering the full data distribution. Specifically, a generator is not penalized for missing a mode. We show that these are therefore still susceptible to not capturing the full data distribution. In this paper, we propose a simple approach that combines an encoder based objective with novel loss functions for generator and discriminator that improves the solution in terms of capturing missing modes. We validate that the proposed method results in substantial improvements through its detailed analysis on toy and real datasets. The quantitative and qualitative results demonstrate that the proposed method improves the solution for the problem of missing modes and improves training of GANs."	"S. Sharma and V.P. Namboodiri,”No Modes left behind: Capturing the data distribution effectively using GANs”, <i> Proceedings of Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18),</i>New Orleans, USA, February 2018"	2018_aaai	http://vinaypn.github.io/files/aaai2018.pdf
2018-03-12	Word spotting in silent lip videos	IEEE Winter Conference on Applications of Computer Vision (WACV)	"Our goal is to spot words in silent speech videos without explicitly recognizing the spoken words, where the lip motion of the speaker is clearly visible and audio is absent. Existing work in this domain has mainly focused on recognizing a fixed set of words in word-segmented lip videos, which limits the applicability of the learned model due to limited vocabulary and high dependency on the model's recognition performance. Our contribution is two-fold: 1) we develop a pipeline for recognition-free retrieval, and show its performance against recognition-based retrieval on a large-scale dataset and another set of out-of-vocabulary words. 2) We introduce a query expansion technique using pseudo-relevant feedback and propose a novel re-ranking method based on maximizing the correlation between spatio-temporal landmarks of the query and the top retrieval candidates. Our word spotting method achieves 35% higher mean average precision over recognition-based method on large-scale LRWdataset. Finally, we demonstrate the application of the method by word spotting in a popular speech video (""The great dictator"" by Charlie Chaplin) where we show that the word retrieval can be used to understand what was spoken perhaps in the silent movies."	"A. Jha, V. P. Namboodiri and C. V. Jawahar, ""Word Spotting in Silent Lip Videos,"" <i>IEEE Winter Conference on Applications of Computer Vision (WACV)</i>, Lake Tahoe, NV, 2018, pp. 150-159."	2018_wacv	http://vinaypn.github.io/files/wacv2018.pdf
2018-06-18	Differential Attention for Visual Question Answering	IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	"In this paper we aim to answer questions based on images when provided with a dataset of question-answer pairs for a number of images during training. A number of methods have focused on solving this problem by using image based attention. This is done by focusing on a specific part of the image while answering the question. Humans also do so when solving this problem. However, the regions that the previous systems focus on are not correlated with the regions that humans focus on. The accuracy is limited due to this drawback. In this paper, we propose to solve this problem by using an exemplar based method. We obtain one or more supporting and opposing exemplars to obtain a differential attention region. This differential attention is closer to human attention than other image based attention methods. It also helps in obtaining improved accuracy when answering questions. The method is evaluated on challenging benchmark datasets. We perform better than other image based attention methods and are competitive with other state of the art methods that focus on both image and questions."	"B.N. Patro and  V.P. Namboodiri, “Differential Attention for Visual Question Answering”, <i>Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>Salt Lake City, Utah, June 2018."	2018_cvpr_badri	https://badripatro.github.io/DVQA/
2018-08-18	Learning semantic sentence embeddings using sequential pair-wise discriminator	International Conference on Computational Linguistics (COLING)	"In this paper, we propose a method for obtaining sentence-level embeddings. While the problem of securing word-level embeddings is very well studied, we propose a novel method for obtaining sentence-level embeddings. This is obtained by a simple method in the context of solving the paraphrase generation task. If we use a sequential encoder-decoder model for generating paraphrase, we would like the generated paraphrase to be semantically close to the original sentence. One way to ensure this is by adding constraints for true paraphrase embeddings to be close and unrelated paraphrase candidate sentence embeddings to be far. This is ensured by using a sequential pair-wise discriminator that shares weights with the encoder that is trained with a suitable loss function. Our loss function penalizes paraphrase sentence embedding distances from being too large. This loss is used in combination with a sequential encoder-decoder network. We also validated our method by evaluating the obtained embeddings for a sentiment analysis task. The proposed method results in semantic embeddings and outperforms the state-of-the-art on the paraphrase generation and sentiment analysis task on standard datasets. These results are also shown to be statistically significant."	"B.N. Patro, V.K. Kurmi, S. Kumar and V.P. Namboodiri, “Learning semantic sentence embeddings using sequential pair-wise discriminator”, <i>Proceedings of the 27th International Conference on Computational Linguistics, COLING 2018,</i> Santa Fe, New Mexico, USA, August 2018"	2018_coling	https://badripatro.github.io/Question-Paraphrases/
2018-06-23	Eclectic domain mixing for effective adaptation in action spaces	Journal on Multimedia Tools and Applications	"Although videos appear to be very high-dimensional in terms of duration × frame-rate × resolution, temporal smoothness constraints ensure that the intrinsic dimensionality for videos is much lower. In this paper, we use this idea for investigating Domain Adaptation (DA) in videos, an area that remains under-explored. An approach that has worked well for the image DA is based on the subspace modeling of the source and target domains, which works under the assumption that the two domains share a latent subspace where the domain shift can be reduced or eliminated. In this paper, first we extend three subspace based image DA techniques for human action recognition and then combine it with our proposed Eclectic Domain Mixing (EDM) approach to improve the effectiveness of the DA. Further, we use discrepancy measures such as Symmetrized KL Divergence and Target Density Around Source for empirical study of the proposed EDM approach. While, this work mainly focuses on Domain Adaptation in videos, for completeness of the study, we comprehensively evaluate our approach using both object and action datasets. In this paper, we have achieved consistent improvements over chosen baselines and obtained some state-of-the-art results for the datasets."	"Jamal, A., Deodhare, D., Namboodiri, V.P., Venkatesh, K.S. “Eclectic domain mixing for effective adaptation in action spaces”, <i> Journal on Multimedia Tools and Applications</i>, November 2018, Volume 77, Issue 22, pp 29949–29969."	2018_mta	https://link.springer.com/article/10.1007/s11042-018-6179-y
2018-04-25	Unsupervised domain adaptation of deep object detectors.	"European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning (ESANN) "	"Domain adaptation has been understood and adopted in vision. Recently with the advent of deep learning there are a number of techniques that propose methods for deep learning based domain adaptation. However, the methods proposed have been used for adapting object classification techniques. In this paper, we solve for domain adaptation of object detection that is more commonly used. We adapt deep adaptation techniques for the Faster R-CNN framework. The techniques that we adapt are the recent techniques based on Gradient Reversal and Maximum Mean Discrepancy (MMD) reduction based techniques. Among them we show that the MK-MMD based method when used appropriately provides the best results. We analyze our model with standard real world settings by using Pascal VOC as source and MS-COCO as target and show a gain of 2.5 mAP at IoU of 0.5 over a source only trained model. We show that this improvement is statistically significant"	"Debjeet Majumdar and Vinay P. Namboodiri,”Unsupervised domain adaptation of deep object detectors.”, <i> 26th European Symposium on Artificial Neural Networks, ESANN 2018, </i>Bruges, Belgium, April 25-27, 2018."	2018_esann	http://vinaypn.github.io/files/esann2018.pdf
2018-09-02	Monoaural Audio Source Separation Using Variational Autoencoders.	Proceedings of Interspeech Conference	"We introduce a monaural audio source separation framework using a latent generative model. Traditionally, discriminative training for source separation is proposed using deep neural networks or non-negative matrix factorization. In this paper, we propose a principled generative approach using variational autoencoders (VAE) for audio source separation. VAE computes efficient Bayesian inference which leads to a continuous latent representation of the input data(spectrogram). It contains a probabilistic encoder which projects an input data to latent space and a probabilistic decoder which projects data from latent space back to input space. This allows us to learn a robust latent representation of sources corrupted with noise and other sources. The latent representation is then fed to the decoder to yield the separated source. Both encoder and decoder are implemented via multilayer perceptron (MLP). In contrast to prevalent techniques, we argue that VAE is a more principled approach to source separation. Experimentally, we find that the proposed framework yields reasonable improvements when compared to baseline methods available in the literature i.e. DNN and RNN with different masking functions and autoencoders. We show that our method performs better than best of the relevant methods with ∼ 2 dB improvement in the source to distortion ratio."	"Laxmi Pandey, Anurendra Kumar and Vinay P. Namboodiri, “Monoaural Audio Source Separation Using Variational Autoencoders.”, <i> 19th Annual Conference of the International Speech Communication Association, Interspeech 2018,</i> Hyderabad, India, 2-6 September 2018"	2018_interspeech	http://vinaypn.github.io/files/interspeech2018.pdf
2018-10-27	Multimodal differential network for visual question generation	Proceedings of Conference on Empirical Methods in Natural Language Processing (EMNLP)	"Generating natural questions from an image is a semantic task that requires using visual and language modality to learn multimodal representations. Images can have multiple visual and language contexts that are relevant for generating questions namely places, captions, and tags. In this paper, we propose the use of exemplars for obtaining the relevant context. We obtain this by using a Multimodal Differential Network to produce natural and engaging questions. The generated questions show a remarkable similarity to the natural questions as validated by a human study. Further, we observe that the proposed approach substantially improves over state-of-the-art benchmarks on the quantitative metrics (BLEU, METEOR, ROUGE, and CIDEr)"	"Badri N. Patro, Sandeep Kumar, Vinod K. Kurmi, Vinay P. Namboodiri,”Multimodal Differential Network for Visual Question Generation”, <i>2018 Conference on Empirical Methods in Natural Language Processing</i>, Brussels, Belgium, 2018"	2018_emnlp	https://badripatro.github.io/MDN-VQG/
2018-09-03	Deep active learning for object detection.	Proceedings of British Machine Vision Conference (BMVC)	"Object detection methods like Single Shot Multibox Detector (SSD) provide highly accurate object detection that run in real-time. However, these approaches require a large number of annotated training images. Evidently, not all of these images are equally useful for training the algorithms. Moreover, obtaining annotations in terms of bounding boxes for each image is costly and tedious. In this paper, we aim to obtain a highly accurate object detector using only a fraction of the training images. We do this by adopting active learning that uses ‘human in the loop’ paradigm to select the set of images that would be useful if annotated. Towards this goal, we make the following contributions: 1. We develop a novel active learning method which poses the layered architecture used in object detection as a ‘query by committee’ paradigm to choose the set of images to be queried. 2. We introduce a framework to use the exploration/exploitation trade-off in our methods. 3. We analyze the results on standard object detection datasets which show that with only a third of the training data, we can obtain more than 95% of the localization accuracy of full supervision. Further our methods outperform classical uncertainty-based active learning algorithms like maximum entropy"	"Soumya Roy, Asim Unmesh and Vinay P. Namboodiri, “Deep active learning for object detection”, <i>British Machine Vision Conference 2018, BMVC 2018</i>, Northumbria University, Newcastle, UK, September 3-6, 2018"	2018_bmvc_soumya	http://bmvc2018.org/contents/papers/0287.pdf
2018-09-03	Deep Domain Adaptation in Action Space.	Proceedings of British Machine Vision Conference (BMVC)	"In the general settings of supervised learning, human action recognition has been a widely studied topic. The classifiers learned in this setting assume that the training and test data have been sampled from the same underlying probability distribution. However, in most of the practical scenarios, this assumption is not true, resulting in a suboptimal performance of the classifiers. This problem, referred to as Domain Shift, has been extensively studied, but mostly for image/object classification task. In this paper, we investigate the problem of Domain Shift in action videos, an area that has remained under-explored, and propose two new approaches named Action Modeling on Latent Subspace (AMLS) and Deep Adversarial Action Adaptation (DAAA). In the AMLS approach, the action videos in the target domain are modeled as a sequence of points on a latent subspace and adaptive kernels are successively learned between the source domain point and the sequence of target domain points on the manifold. In the DAAA approach, an end-to-end adversarial learning framework is proposed to align the two domains. The action adaptation experiments were conducted using various combinations of multi-domain action datasets, including six common classes of Olympic Sports and UCF50 datasets and all classes of KTH, MSR and our own SonyCam datasets. In this paper, we have achieved consistent improvements over chosen baselines and obtained some state-of-the-art results for the datasets."	"Arshad Jamal, Vinay P. Namboodiri, Dipti Deodhare and K.S. Venkatesh, “Deep Domain Adaptation in Action Space”, <i>British Machine Vision Conference 2018, BMVC 2018</i>, Northumbria University, Newcastle, UK, September 3-6, 2018"	2018_bmvc_arshad	http://bmvc2018.org/contents/papers/0960.pdf
2018-12-02	U-DADA: Unsupervised Deep Action Domain Adaptation	Asian Conference on Computer Vision (ACCV)	"The problem of domain adaptation has been extensively studied for object classification task. However, this problem has not been as well studied for recognizing actions. While, object recognition is well understood, the diverse variety of videos in action recognition make the task of addressing domain shift to be more challenging. We address this problem by proposing a new novel adaptation technique that we term as unsupervised deep action domain adaptation (U-DADA). The main concept that we propose is that of explicitly modeling density based adaptation and using them while adapting domains for recognizing actions. We show that these techniques work well both for domain adaptation through adversarial learning to obtain invariant features or explicitly reducing the domain shift between distributions. The method is shown to work well using existing benchmark datasets such as UCF50, UCF101, HMDB51 and Olympic Sports. As a pioneering effort in the area of deep action adaptation, we are presenting several benchmark results and techniques that could serve as baselines to guide future research in this area."	"Jamal A., Namboodiri V.P., Deodhare D., Venkatesh K.S. (2019) U-DADA: Unsupervised Deep Action Domain Adaptation. In: Jawahar C., Li H., Mori G., Schindler K. (eds) Computer Vision – ACCV 2018. ACCV 2018. Lecture Notes in Computer Science, vol 11363. Springer,"	2018_accv	https://link.springer.com/chapter/10.1007/978-3-030-20893-6_28
2018-12-18	Supervised Hashing for Retrieval of Multimodal Biometric Data	3rd Workshop on Computer Vision Applications (WCVA)	"Biometric systems commonly utilize multi-biometric approaches where a person is verified or identified based on multiple biometric traits. However, requiring systems that are deployed usually require verification or identification from a large number of enrolled candidates. These are possible only if there are efficient methods that retrieve relevant candidates in a multi-biometric system. To solve this problem, we analyze the use of hashing techniques that are available for obtaining retrieval. We specifically based on our analysis recommend the use of supervised hashing techniques over deep learned features as a possible common technique to solve this problem. Our investigation includes a comparison of some of the supervised and unsupervised methods viz. Principal Component Analysis (PCA), Locality Sensitive Hashing (LSH), Locality-sensitive binary codes from shift-invariant kernels (SKLSH), Iterative quantization: A procrustean approach to learning binary codes (ITQ), Binary Reconstructive Embedding (BRE) and Minimum loss hashing (MLH) that represent the prevalent classes of such systems and we present our analysis for the following biometric data: Face, Iris, and Fingerprint for a number of standard datasets. The main technical contributions through this work are as follows: (a) Proposing Siamese network based deep learned feature extraction method (b) Analysis of common feature extraction techniques for multiple biometrics as to a reduced feature space representation (c) Advocating the use of supervised hashing for obtaining a compact feature representation across different biometrics traits. (d) Analysis of the performance of deep representations against shallow representations in a practical reduced feature representation framework. Through experimentation with multiple biometrics traits, feature representations, and hashing techniques, we can conclude that current deep learned features when retrieved using supervised hashing can be a standard pipeline adopted for most unimodal and multimodal biometric identification tasks."	"Sumesh T.A., Namboodiri V., Gupta P. (2019) Supervised Hashing for Retrieval of Multimodal Biometric Data. In: Arora C., Mitra K. (eds) Computer Vision Applications. WCVA 2018. Communications in Computer and Information Science, vol 1019. Springer, Singapore"	2018_wcva	https://link.springer.com/chapter/10.1007/978-981-15-1387-9_8
2019-01-08	Multi-layer pruning framework for compressing single shot multibox detector	IEEE Winter Conference on Applications of Computer Vision (WACV)	"We propose a framework for compressing state-of-the-art Single Shot MultiBox Detector (SSD). The framework addresses compression in the following stages: Sparsity Induction, Filter Selection, and Filter Pruning. In the Sparsity Induction stage, the object detector model is sparsified via an improved global threshold. In Filter Selection & Pruning stage, we select and remove filters using sparsity statistics of filter weights in two consecutive convolutional layers. This results in the model with the size smaller than most existing compact architectures. We evaluate the performance of our framework with multiple datasets and compare over multiple methods. Experimental results show that our method achieves state-of-the-art compression of 6.7X and 4.9X on PASCAL VOC dataset on models SSD300 and SSD512 respectively. We further show that the method produces maximum compression of 26X with SSD512 on German Traffic Sign Detection Benchmark (GTSDB). Additionally, we also empirically show our method's adaptability for classification based architecture VGG16 on datasets CIFAR and German Traffic Sign Recognition Benchmark (GTSRB) achieving a compression rate of 125X and 200X with the reduction in flops by 90.50% and 96.6% respectively with no loss of accuracy. In addition to this, our method does not require any special libraries or hardware support for the resulting compressed models."	"P. Singh, Manikandan R., N. Matiyali and V. P. Namboodiri, ""Multi-layer pruning framework for compressing single shot multibox detector,"" <i>IEEE Winter Conference on Applications of Computer Vision (WACV)</i>, Waikoloa Village, Hawaii, USA."	2019_wacv_pravendra_a	https://arxiv.org/abs/1811.08342
2019-01-08	Stability Based Filter Pruning for Accelerating Deep CNNs	IEEE Winter Conference on Applications of Computer Vision (WACV)	"Convolutional neural networks (CNN) have achieved impressive performance on the wide variety of tasks (classification, detection, etc.) across multiple domains at the cost of high computational and memory requirements. Thus, leveraging CNNs for real-time applications necessitates model compression approaches that not only reduce the total number of parameters but reduce the overall computation as well. In this work, we present a stability-based approach for filter-level pruning of CNNs. We evaluate our proposed approach on different architectures (LeNet, VGG-16, ResNet, and Faster RCNN) and datasets and demonstrate its generalizability through extensive experiments. Moreover, our compressed models can be used at run-time without requiring any special libraries or hardware. Our model compression method reduces the number of FLOPS by an impressive factor of 6.03X and GPU memory footprint by more than 17X, significantly outperforming other state-of-the-art filter pruning methods."	"P. Singh, Manikandan V.S.R. Kadi, N. Verma and V. P. Namboodiri, ""Stability Based Filter Pruning for Accelerating Deep CNNs,"" <i>IEEE Winter Conference on Applications of Computer Vision (WACV)</i>, Waikoloa Village, Hawaii, USA."	2019_wacv_pravendra_b	https://arxiv.org/abs/1811.08321
2019-02-08	Spotting words in silent speech videos: a retrieval-based approach	Journal of Machine Vision and Applications (MVA)	"Our goal is to spot words in silent speech videos without explicitly recognizing the spoken words, where the lip motion of the speaker is clearly visible and audio is absent. Existing work in this domain has mainly focused on recognizing a fixed set of words in word-segmented lip videos, which limits the applicability of the learned model due to limited vocabulary and high dependency on the model’s recognition performance. Our contribution is twofold: (1) we develop a pipeline for recognition-free retrieval and show its performance against recognition-based retrieval on a large-scale dataset and another set of out-of-vocabulary words. (2) We introduce a query expansion technique using pseudo-relevant feedback and propose a novel re-ranking method based on maximizing the correlation between spatiotemporal landmarks of the query and the top retrieval candidates. Our word spotting method achieves 35% higher mean average precision over recognition-based method on large-scale LRW dataset. We also demonstrate the application of the method by word spotting in a popular speech video (“The great dictator” by Charlie Chaplin) where we show that the word retrieval can be used to understand what was spoken perhaps in the silent movies. Finally, we compare our model against ASR in a noisy environment and analyze the effect of the performance of underlying lip-reader and input video quality on the proposed word spotting pipeline."	"A. Jha, V. P. Namboodiri and C. V. Jawahar,”Spotting words in silent speech videos: a retrieval-based approach”, <i>Journal of Machine Vision and Applications</i>, March 2019, Volume 30, Issue 2, pp 217–229"	2019_mva	https://link.springer.com/article/10.1007/s00138-019-01006-y
2019-06-16	Hetconv: Heterogeneous kernel-based convolutions for deep cnns	IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	"We present a novel deep learning architecture in which the convolution operation leverages heterogeneous kernels. The proposed HetConv (Heterogeneous Kernel-Based Convolution) reduces the computation (FLOPs) and the number of parameters as compared to standard convolution operation while still maintaining representational efficiency. To show the effectiveness of our proposed convolution, we present extensive experimental results on the standard convolutional neural network (CNN) architectures such as VGG \cite{vgg2014very} and ResNet \cite{resnet}. We find that after replacing the standard convolutional filters in these architectures with our proposed HetConv filters, we achieve 3X to 8X FLOPs based improvement in speed while still maintaining (and sometimes improving) the accuracy. We also compare our proposed convolutions with group/depth wise convolutions and show that it achieves more FLOPs reduction with significantly higher accuracy."	"Pravendra Singh, Vinay Kumar Verma, Piyush Rai and Vinay P. Namboodiri,”Hetconv: Heterogeneous kernel-based convolutions for deep cnns”, <i>Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>Long Beach, California, June 2019."	2019_cvpr_pravendra	http://openaccess.thecvf.com/content_CVPR_2019/papers/Singh_HetConv_Heterogeneous_Kernel-Based_Convolutions_for_Deep_CNNs_CVPR_2019_paper.pdf
2019-06-16	Attending to Discriminative Certainty for Domain Adaptation	IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	"In this paper, we aim to solve for unsupervised domain adaptation of classifiers where we have access to label information for the source domain while these are not available for a target domain. While various methods have been proposed for solving these including adversarial discriminator based methods, most approaches have focused on the entire image based domain adaptation. In an image, there would be regions that can be adapted better, for instance, the foreground object may be similar in nature. To obtain such regions, we propose methods that consider the probabilistic certainty estimate of various regions and specify focus on these during classification for adaptation. We observe that just by incorporating the probabilistic certainty of the discriminator while training the classifier, we are able to obtain state of the art results on various datasets as compared against all the recent methods. We provide a thorough empirical analysis of the method by providing ablation analysis, statistical significance test, and visualization of the attention maps and t-SNE embeddings. These evaluations convincingly demonstrate the effectiveness of the proposed approach."	"Vinod Kumar Kurmi*, Shanu Kumar* and Vinay P Namboodiri,”Attending to Discriminative Certainty for Domain Adaptation”, <i>Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>Long Beach, California, June 2019."	2019_cvpr_vinod_shanu	http://openaccess.thecvf.com/content_CVPR_2019/papers/Kurmi_Attending_to_Discriminative_Certainty_for_Domain_Adaptation_CVPR_2019_paper.pdf
2019-05-12	Cross-language Speech Dependent Lip-synchronization	"IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"	"Understanding videos of people speaking across international borders is hard as audiences from different demographies do not understand the language. Such speech videos are often supplemented with language subtitles. However, these hamper the viewing experience as the attention is shared. Simple audio dubbing in a different language makes the video appear unnatural due to unsynchronized lip motion. In this paper, we propose a system for automated cross-language lip synchronization for re-dubbed videos. Our model generates superior photorealistic lip-synchronization over original video in comparison to the current re-dubbing method. With the help of a user-based study, we verify that our method is preferred over unsynchronized videos."	"A. Jha, V. Voleti, V. Namboodiri and C. V. Jawahar, ""Cross-language Speech Dependent Lip-synchronization,"" ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Brighton, United Kingdom, 2019, pp. 7140-7144."	2019_icassp	http://vinaypn.github.io/files/icassp2019.pdf
2019-07-14	Looking back at Labels: A Class based Domain Adaptation Technique	International Joint Conference on Neural Networks (IJCNN) 	"In this paper, we tackle a problem of Domain Adaptation. In a domain adaptation setting, there is provided a labeled set of examples in a source dataset with multiple classes being present and a target dataset that has no supervision. In this setting, we propose an adversarial discriminator based approach. While the approach based on adversarial discriminator has been previously proposed; in this paper, we present an informed adversarial discriminator. Our observation relies on the analysis that shows that if the discriminator has access to all the information available including the class structure present in the source dataset, then it can guide the transformation of features of the target set of classes to a more structured adapted space. Using this formulation, we obtain the state-of-the-art results for the standard evaluation on benchmark datasets. We further provide detailed analysis which shows that using all the labeled information results in an improved domain adaptation."	"Vinod Kumar Kurmi and Vinay P. Namboodiri, “Looking back at Labels: A Class based Domain Adaptation Technique”, <i> Proceedings of International Joint Conference on Neural Networks (IJCNN)</i>, Budapest, Hungary"	2019_ijcnn_vinod	https://vinodkkurmi.github.io/DiscriminatorDomainAdaptation/
2019-07-14	Unsupervised Synthesis of Anomalies in Videos: Transforming the Normal	International Joint Conference on Neural Networks (IJCNN) 	"Abnormal activity recognition requires detection of occurrence of anomalous events that suffer from a severe imbalance in data. In a video, normal is used to describe activities that conform to usual events while the irregular events which do not conform to the normal are referred to as abnormal. It is far more common to observe normal data than to obtain abnormal data in visual surveillance. In this paper, we propose an approach where we can obtain abnormal data by transforming normal data. This is a challenging task that is solved through a multi-stage pipeline approach. We utilize a number of techniques from unsupervised segmentation in order to synthesize new samples of data that are transformed from an existing set of normal examples. Further, this synthesis approach has useful applications as a data augmentation technique. An incrementally trained Bayesian convolutional neural network (CNN) is used to carefully select the set of abnormal samples that can be added. Finally through this synthesis approach we obtain a comparable set of abnormal samples that can be used for training the CNN for the classification of normal vs abnormal samples. We show that this method generalizes to multiple settings by evaluating it on two real world datasets and achieves improved performance over other probabilistic techniques that have been used in the past for this task."	"Abhishek Joshi and Vinay P. Namboodiri,”Unsupervised Synthesis of Anomalies in Videos: Transforming the Normal”, <i> Proceedings of International Joint Conference on Neural Networks (IJCNN)</i>, Budapest, Hungary"	2019_ijcnn_abhishek	https://arxiv.org/abs/1904.06633
2019-08-10	Play and Prune: Adaptive Filter Pruning for Deep Model Compression	International Joint Conference on Artificial Intelligence (IJCAI-2019)	"While convolutional neural networks (CNN) have achieved impressive performance on various classification/recognition tasks, they typically consist of a massive number of parameters. This results in significant memory requirement as well as computational overheads. Consequently, there is a growing need for filter-level pruning approaches for compressing CNN based models that not only reduce the total number of parameters but reduce the overall computation as well. We present a new min-max framework for filter-level pruning of CNNs. Our framework, called Play and Prune (PP), jointly prunes and fine-tunes CNN model parameters, with an adaptive pruning rate, while maintaining the model's predictive performance. Our framework consists of two modules: (1) An adaptive filter pruning (AFP) module, which minimizes the number of filters in the model; and (2) A pruning rate controller (PRC) module, which maximizes the accuracy during pruning. Moreover, unlike most previous approaches, our approach allows directly specifying the desired error tolerance instead of pruning level. Our compressed models can be deployed at run-time, without requiring any special libraries or hardware. Our approach reduces the number of parameters of VGG-16 by an impressive factor of 17.5X, and number of FLOPS by 6.43X, with no loss of accuracy, significantly outperforming other state-of-the-art filter pruning methods."	"Pravendra Singh, Vinay Kumar Verma, Piyush Rai and Vinay P. Namboodiri,”Play and Prune: Adaptive Filter Pruning for Deep Model Compression”, <i>Proceedings of International Joint Conference on Artificial Intelligence (IJCAI-2019)</i>Macao, China, August 2019."	2019_ijcai	https://arxiv.org/abs/1905.04446
2019-09-09	Curriculum based Dropout Discriminator for Domain Adaptation	Proceedings of British Machine Vision Conference (BMVC)	"Domain adaptation is essential to enable wide usage of deep learning based networks trained using large labeled datasets. Adversarial learning based techniques have shown their utility towards solving this problem using a discriminator that ensures source and target distributions are close. However, here we suggest that rather than using a point estimate, it would be useful if a distribution based discriminator could be used to bridge this gap. This could be achieved using multiple classifiers or using traditional ensemble methods. In contrast, we suggest that a Monte Carlo dropout based ensemble discriminator could suffice to obtain the distribution based discriminator. Specifically, we propose a curriculum based dropout discriminator that gradually increases the variance of the sample based distribution and the corresponding reverse gradients are used to align the source and target feature representations. The detailed results and thorough ablation analysis show that our model outperforms state-of-the-art results."	"Vinod Kumar Kurmi, Vipul Bajaj, Venkatesh K Subramanian and Vinay P Namboodiri, “Curriculum based Dropout Discriminator for Domain Adaptation”, <i>British Machine Vision Conference 2018, BMVC 2018</i>, Cardiff, UK, Northumbria University, Newcastle, UK, September 9-12, 2019"	2019_bmvc	https://delta-lab-iitk.github.io/CD3A/
2019-10-27	U-CAM: Visual Explanation using Uncertainty based Class Activation Maps	IEEE International Conference on Computer Vision (ICCV)	"Understanding and explaining deep learning models is an imperative task. Towards this, we propose a method that obtains gradient-based certainty estimates that also provide visual attention maps. Particularly, we solve for visual question answering task. We incorporate modern probabilistic deep learning methods that we further improve by using the gradients for these estimates. These have two-fold benefits: a) improvement in obtaining the certainty estimates that correlate better with misclassified samples and b) improved attention maps that provide state-of-the-art results in terms of correlation with human attention regions. The improved attention maps result in consistent improvement for various methods for visual question answering. Therefore, the proposed technique can be thought of as a recipe for obtaining improved certainty estimates and explanation for deep learning models. We provide detailed empirical analysis for the visual question answering task on all standard benchmarks and comparison with state of the art methods."	"Badri N. Patro, Mayank Lunayach, Shivansh Patel and Vinay P. Namboodiri, “U-CAM: Visual Explanation using Uncertainty based Class Activation Maps”, <i>Proceedings of IEEE International Conference on Computer Vision (ICCV)</i>Seoul, South Korea, October 2019."	2019_iccv	https://delta-lab-iitk.github.io/U-CAM/
2019-10-21	Towards Automatic Face-to-Face Translation	27th ACM International Conference on Multimedia (ACM-MM)	"In light of the recent breakthroughs in automatic machine translation systems, we propose a novel approach that we term as ""Face-to-Face Translation"". As today's digital communication becomes increasingly visual, we argue that there is a need for systems that can automatically translate a video of a person speaking in language A into a target language B with realistic lip synchronization. In this work, we create an automatic pipeline for this problem and demonstrate its impact in multiple real-world applications. First, we build a working speech-to-speech translation system by bringing together multiple existing modules from speech and language. We then move towards ""Face-to-Face Translation"" by incorporating a novel visual module, LipGAN for generating realistic talking faces from the translated audio. Quantitative evaluation of LipGAN on the standard LRW test set shows that it significantly outperforms existing approaches across all standard metrics. We also subject our Face-to-Face Translation pipeline, to multiple human evaluations and show that it can significantly improve the overall user experience for consuming and interacting with multimodal content across languages. Code, models and demo video are made publicly available."	"Prajwal Renukanand*, Rudrabha Mukhopadhyay*, Jerin Philip, Abhishek Jha, Vinay Namboodiri and C.V. Jawahar, “Towards Automatic Face-to-Face Translation”, <i> 27th ACM International Conference on Multimedia (ACM-MM),</i> Nice, France, 2019, Pages 1428–1436"	2019_acmmm	https://cvit.iiit.ac.in/research/projects/cvit-projects/facetoface-translation
2019-11-18	HetConv: Beyond Homogeneous Convolution Kernels for Deep CNNs	International Journal of Computer Vision (IJCV)	"While usage of convolutional neural networks (CNN) is widely prevalent, methods proposed so far always have considered homogeneous kernels for this task. In this paper, we propose a new type of convolution operation using heterogeneous kernels. The proposed Heterogeneous Kernel-Based Convolution (HetConv) reduces the computation (FLOPs) and the number of parameters as compared to standard convolution operation while it maintains representational efficiency. To show the effectiveness of our proposed convolution, we present extensive experimental results on the standard CNN architectures such as VGG, ResNet, Faster-RCNN, MobileNet, and SSD. We observe that after replacing the standard convolutional filters in these architectures with our proposed HetConv filters, we achieve 1.5 × to 8 × FLOPs based improvement in speed while it maintains (sometimes improves) the accuracy. We also compare our proposed convolution with group/depth wise convolution and show that it achieves more FLOPs reduction with significantly higher accuracy. Moreover, we demonstrate the efficacy of HetConv based CNN by showing that it also generalizes on object detection and is not constrained to image classification tasks. We also empirically show that the proposed HetConv convolution is more robust towards the over-fitting problem as compared to standard convolution."	"Pravendra Singh, Vinay Kumar Verma, Piyush Rai and Vinay P. Namboodiri, “HetConv: Beyond Homogeneous Convolution Kernels for Deep CNNs”, <i>International Journal of Computer Vision</i>, accepted"	2019_ijcv	https://link.springer.com/article/10.1007/s11263-019-01264-3
2019-11-27	FALF ConvNets: Fatuous auxiliary loss based filter-pruning for efficient deep CNNs	Image and Vision Computing Journal	"Obtaining efficient Convolutional Neural Networks (CNNs) are imperative to enable their application for a wide variety of tasks (classification, detection, etc.). While several methods have been proposed to solve this problem, we propose a novel strategy for solving the same that is orthogonal to the strategies proposed so far. We hypothesize that if we add a fatuous auxiliary task, to a network which aims to solve a semantic task such as classification or detection, the filters devoted to solving this frivolous task would not be relevant for solving the main task of concern. These filters could be pruned and pruning these would not reduce the performance on the original task. We demonstrate that this strategy is not only successful, it in fact allows for improved performance for a variety of tasks such as object classification, detection and action recognition. An interesting observation is that the task needs to be fatuous so that any semantically meaningful filters would not be relevant for solving this task. We thoroughly evaluate our proposed approach on different architectures (LeNet, VGG-16, ResNet, Faster RCNN, SSD-512, C3D, and MobileNet V2) and datasets (MNIST, CIFAR, ImageNet, GTSDB, COCO, and UCF101) and demonstrate its generalizability through extensive experiments. Moreover, our compressed models can be used at run-time without requiring any special libraries or hardware. Our model compression method reduces the number of FLOPS by an impressive factor of 6.03X and GPU memory footprint by more than 17X for VGG-16, significantly outperforming other state-of-the-art filter pruning methods. We demonstrate the usability of our approach for 3D convolutions and various vision tasks such as object classification, object detection, and action recognition."	"Pravendra Singh, Vinay Sameer Raja Kadi and Vinay P.Namboodiri, “FALF ConvNets: Fatuous auxiliary loss based filter-pruning for efficient deep CNNs”, <i>Image and Vision Computing Journal</i>, Volume 93, January 2020, 103857"	2019_ivc	https://www.sciencedirect.com/science/article/pii/S0262885619304500
2021-01-06	"SHAD3S: A model to Sketch, Shade and Shadow"	2021 IEEE Winter Conference for Applications on Computer Vision (WACV)	"Hatching is a common method used by artists to accentuate the third dimension of a sketch, and to illuminate the scene. Our system attempts to compete with a human at hatching generic three-dimensional (3d) shapes, and also tries to assist her in a form exploration exercise. The novelty of our approach lies in the fact that we make no assumptions about the input other than that it represents a 3d shape, and yet, given a contextual information of illumination and texture, we synthesise an accurate hatch pattern over the sketch, without access to 3d or pseudo 3d. In the process, we contribute towards a) a cheap yet effective method to synthesise a sufficiently large high fidelity dataset, pertinent to task; b) creating a pipeline with conditional generative adversarial network (cgan); and c) creating an interactive utility with gimp, that is a tool for artists to engage with automated hatching or a form-exploration exercise. User evaluation of the tool suggests that the model performance does generalise satisfactorily over diverse input, both in terms of style as well as shape. A simple comparison of inception scores suggest that the generated distribution is as diverse as the ground truth."	"R. B. Venkataramaiyer, A. Joshi, S. Narang, and V. P. Namboodiri (2021). ""SHAD3S : A model to Sketch, Shade and Shadow"", <i>Proceedings of 2021 IEEE Winter Conference on Applications of Computer Vision (WACV)""</i> Hawaii, USA, Jan. 2021."	2021_wacv_bvraghav	https://bvraghav.com/shad3s/
2020-03-01	Can I teach a robot to replicate a line art 	2020 IEEE Winter Conference on Applications of Computer Vision (WACV)	"Line art is arguably one of the fundamental and versatile modes of expression. We propose a pipeline for a robot to look at a grayscale line art and redraw it. The key novel elements of our pipeline are: a) we propose a novel task of mimicking line drawings, b) to solve the pipeline we modify the Quick-draw dataset to obtain supervised training for converting a line drawing into a series of strokes c) we propose a multi-stage segmentation and graph interpretation pipeline for solving the problem. The resultant method has also been deployed on a CNC plotter as well as a robotic arm. We have trained several variations of the proposed methods and evaluate these on a dataset obtained from Quick-draw. Through the best methods we observe an accuracy of around 98% for this task, which is a significant improvement over the baseline architecture we adapted from. This therefore allows for deployment of the method on robots for replicating line art in a reliable manner. We also show that while the rule-based vectorization methods do suffice for simple drawings, it fails for more complicated sketches, unlike our method which generalizes well to more complicated distributions."	"R. B. Venkataramaiyer, S. Kumar, and V. P. Namboodiri (2020). ""Can I teach a robot to replicate a line art "", <i>Proceedings of 2020 IEEE Winter Conference on Applications of Computer Vision (WACV)""</i> Aspen, USA, Mar. 2020."	2020_wacv_bvraghav	https://bvraghav.com/can-i-teach-a-robot-to-replicate-a-line-art/
2021-01-05	Domain Impression: A Source Data Free Domain Adaptation Method	Winter Conference on Applications of Computer Vision (WACV)	"Unsupervised Domain adaptation methods solve the adaptation problem for an unlabeled target set, assuming that the source dataset is available with all labels. However, the availability of actual source samples is not always possible in practical cases. It could be due to memory constraints, privacy concerns, and challenges in sharing data. This practical scenario creates a bottleneck in the domain adaptation problem. This paper addresses this challenging scenario by proposing a domain adaptation technique that does not need any source data. Instead of the source data, we are only provided with a classifier that is trained on the source data. Our proposed approach is based on a generative framework, where the trained classifier is used for generating samples from the source classes. We learn the joint distribution of data by using the energy-based modeling of the trained classifier. At the same time, a new classifier is also adapted for the target domain. We perform various ablation analysis under different experimental setups and demonstrate that the proposed approach achieves better results than the baseline models in this extremely novel scenario."	"Vinod K. Kurmi, Venkatesh K Subramanian, Vinay P. Namboodiri, “Domain Impression: A Source Data Free Domain Adaptation Method”,  <i>IEEE Winter Conference of Applications on Computer Vision (WACV), Virtual, 2021</i>"	2021_wacv_vinod_1	https://delta-lab-iitk.github.io/SFDA/
2021-01-05	 Do not Forget to Attend to Uncertainty while Mitigating Catastrophic Forgetting	Winter Conference on Applications of Computer Vision (WACV )	"One of the major limitations of deep learning models is that they face catastrophic forgetting in an incremental learning scenario.  There have been several approaches proposed to tackle the problem of incremental learning. Most of these methods are based on knowledge distillation and do not adequately utilize the information provided by older task models, such as uncertainty estimation in predictions. The predictive uncertainty provides the distributional information can be applied to mitigate catastrophic forgetting in a deep learning framework.  In the proposed work, we consider a Bayesian formulation to obtain the data and model uncertainties. We also incorporate self-attention framework to address the incremental learning problem. We define distillation losses in terms of aleatoric uncertainty and self-attention. In the proposed work, we investigate different ablation analyses on these losses. Furthermore, we are able to obtain better results in terms of accuracy on standard benchmarks."	"Vinod K. Kurmi, Badri N. Patro, Venkatesh K Subramanian, Vinay P. Namboodiri,“Do not Forget to Attend to Uncertainty while Mitigating Catastrophic Forgetting”, <i>IEEE Winter Conference of Applications on Computer Vision (WACV), Virtual, 2021 </i>"	2021_wacv_vinod_2	https://delta-lab-iitk.github.io/Incremental-learning-AU/
2020-03-01	Accuracy booster: Performance boosting using feature map re-calibration	 Winter Conference on Applications of Computer Vision (WACV)	"Convolution Neural Networks (CNN) have been extremely successful in solving intensive computer vision tasks. The convolutional filters used in CNNs have played a major role in this success, by extracting useful features from the inputs. Recently researchers have tried to boost the performance of CNNs by re-calibrating the feature maps produced by these filters, eg, Squeeze-and-Excitation Networks (SENets). These approaches have achieved better performance by Exciting up the important channels or feature maps while diminishing the rest. However, in the process, architectural complexity has increased. We propose an architectural block that introduces much lower complexity than the existing methods of CNN performance boosting while performing significantly better than them. We carry out experiments on the CIFAR, ImageNet and MS-COCO datasets, and show that the proposed block can challenge the state-of-the-art results. Our method boosts the ResNet-50 architecture to perform comparably to the ResNet-152 architecture, which is a three times deeper network, on classification. We also show experimentally that our method is not limited to classification but also generalizes well to other tasks such as object detection."	"Pravendra Singh, Pratik Mazumder and Vinay P. Namboodiri, “Accuracy booster: Performance boosting using feature map re-calibration”,  <i> The IEEE Winter Conference on Applications of Computer Vision, pages=884--893, 2020 </i>"	singh2020accuracy	http://openaccess.thecvf.com/content_WACV_2020/papers/Singh_Accuracy_Booster_Performance_Boosting_using_Feature_Map_Re-calibration_WACV_2020_paper.pdf
2020-05-04	Cpwc: Contextual point wise convolution for object recognition	"IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"	"Convolutional layers are a major driving force behind the successes of deep learning. Pointwise convolution (PWC) is a 1 × 1 convolutional filter that is primarily used for parameter reduction. However, the PWC ignores the spatial information around the points it is processing. This design is by choice, in order to reduce the overall parameters and computations. However, we hypothesize that this shortcoming of PWC has a significant impact on the network performance. We propose an alternative design for pointwise convolution, which uses spatial information from the input efficiently. Our design significantly improves the performance of the networks without substantially increasing the number of parameters and computations. We experimentally show that our design results in significant improvement in the performance of the network for classification as well as detection."	"Pravendra Singh, Pratik Mazumder and Vinay P. Namboodiri, “Cpwc: Contextual point wise convolution for object recognition”,<i> ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) ,pages=4152–4156, 2020 </i>"	mazumder2020cpwc	https://ieeexplore.ieee.org/document/9054205
2020-12-01	GIFSL-grafting based improved few-shot learning	Image and Vision Computing	"A few-shot learning model generally consists of a feature extraction network and a classification module. In this paper, we propose an approach to improve few-shot image classification performance by increasing the representational capacity of the feature extraction network and improving the quality of the features extracted by it. The ability of the feature extraction network to extract highly discriminative features from images is essential to few-shot learning. Such features are generally class agnostic and contain information about the general content of the image. Our approach improves the training of the feature extraction network in order to enable them to produce such features. We train the network using filter-grafting along with an auxiliary self-supervision task and a knowledge distillation procedure. Particularly, filter-grafting rejuvenates unimportant (invalid) filters in the feature extraction network to make them useful and thereby, increases the number of important filters that can be further improved by using self-supervision and knowledge distillation techniques. This combined approach helps in significantly improving the few-shot learning performance of the model. We perform experiments on several few-shot learning benchmark datasets such as mini-ImageNet, tiered-ImageNet, CIFAR-FS, and FC100 using our approach. We also present various ablation studies to validate the proposed approach. We empirically show that our approach performs better than other state-of-the-art few-shot learning methods."	"Pratik Mazumder, Pravendra Singh and Vinay P. Namboodiri, ”GIFSL - grafting based improved few-shot learning"", <i> Journal of Image and Vision Computing, volume 104 2020 </i>, issn = ""0262-8856"", doi = ""https://doi.org/10.1016/j.imavis.2020.104006"""	MAZUMDER2020104006	https://www.sciencedirect.com/science/article/abs/pii/S0262885620301384
2020-10-25	Stochastic Talking Face Generation Using Latent Distribution Matching	INTERSPEECH	"The ability to envisage the visual of a talking face based just on hearing a voice is a unique human capability. There have been a number of works that have solved for this ability recently. We differ from these approaches by enabling a variety of talking face generations based on single audio input. Indeed, just having the ability to generate a single talking face would make a system almost robotic in nature. In contrast, our unsupervised stochastic audio-to-video generation model allows for diverse generations from a single audio input. Particularly, we present an unsupervised stochastic audio-to-video generation model that can capture multiple modes of the video distribution. We ensure that all the diverse generations are plausible. We do so through a principled multi-modal variational autoencoder framework. We demonstrate its efficacy on the challenging LRW and GRID datasets and demonstrate performance better than the baseline, while having the ability to generate multiple diverse lip synchronized videos."	"Ravindra Yadav, Ashish Sardana, Vinay P Namboodiri, Rajesh M Hegde. ”Stochastic Talking Face Generation Using Latent Distribution Matching.” In InterSpeech, Shanghai, China. October 25, 2020."	2020_interspeech_speech2face	https://www.isca-speech.org/archive/Interspeech_2020/pdfs/1823.pdf
2021-01-05	RNNP: A Robust Few-Shot Learning Approach	Winter Conference on Applications of Computer Vision (WACV) 2021	"Learning from a few examples is an important practical aspect of training classifiers. Various works have examined this aspect quite well. However, all existing approaches assume that the few examples provided are always correctly labeled. This is a strong assumption, especially if one considers the current techniques for labeling using crowd-based labeling services. We address this issue by proposing a novel robust few-shot learning approach. Our method relies on generating robust prototypes from a set of few examples. Specifically, our method refines the class prototypes by producing hybrid features from the support examples of each class. The refined prototypes help to classify the query images better. Our method can replace the evaluation phase of any few-shot learning method that uses a nearest neighbor prototype-based evaluation procedure to make them robust. We evaluate our method on standard mini-ImageNet and tiered-ImageNet datasets. We perform experiments with various label corruption rates in the support examples of the few-shot classes. We obtain significant improvement over widely used few-shot learning methods that suffer significant performance degeneration in the presence of label noise. We finally provide extensive ablation experiments to validate our method."	"Pratik Mazumder, Pravendra Singh and Vinay P. Namboodiri, “RNNP: A Robust Few-Shot Learning Approach”, <i> The IEEE Winter Conference on Applications of Computer Vision, WACV 2021</i>"	mazumder2020rnnp	https://arxiv.org/abs/2011.11067
2021-01-05	AVGZSLNet: Audio-Visual Generalized Zero-Shot Learning by Reconstructing Label Features from Multi-Modal Embeddings	Winter Conference on Applications of Computer Vision (WACV) 2021	"In this paper, we solve for the problem of generalized zero-shot learning in a multi-modal setting, where we have novel classes of audio/video during testing that were not seen during training. We demonstrate that projecting the audio and video embeddings to the class label text feature space allows us to use the semantic relatedness of text embeddings as a means for zero-shot learning. Importantly, our multi-modal zero-shot learning approach works even if a modality is missing at test time. Our approach makes use of a cross-modal decoder which enforces the constraint that the class label text features can be reconstructed from the audio and video embeddings of data points in order to perform better on the multi-modal zero-shot learning task. We further minimize the gap between audio and video embedding distributions using KL-Divergence loss. We test our approach on the zero-shot classification and retrieval tasks, and it performs better than other models in the presence of a single modality as well as in the presence of multiple modalities."	"Pratik Mazumder and Pravendra Singh, Kranti Kumar Parida and Vinay P. Namboodiri, “AVGZSLNet: Audio-Visual Generalized Zero-Shot Learning by Reconstructing Label Features from Multi-Modal Embeddings”, <i> The IEEE Winter Conference on Applications of Computer Vision, WACV 2021</i>"	mazumder2020avgzslnet	https://arxiv.org/abs/2005.13402
2021-01-05	Improving Few-Shot Learning using Composite Rotation based Auxiliary Task	Winter Conference on Applications of Computer Vision (WACV) 2021	"In this paper, we propose an approach to improve few-shot classification performance using a composite rotation based auxiliary task. Few-shot classification methods aim to produce neural networks that perform well for classes with a large number of training samples and classes with less number of training samples. They employ techniques to enable the network to produce highly discriminative features that are also very generic. Generally, the better the quality and generic-nature of the features produced by the network, the better is the performance of the network on few-shot learning. Our approach aims to train networks to produce such features by using a self-supervised auxiliary task. Our proposed composite rotation based auxiliary task performs rotation at two levels, ie, rotation of patches inside the image (inner rotation) and rotation of the whole image (outer rotation) and assigns one out of 16 rotation classes to the modified image. We then simultaneously train for the composite rotation prediction task along with the original classification task, which forces the network to learn high-quality generic features that help improve the few-shot classification performance. We experimentally show that our approach performs better than existing few-shot learning methods on multiple benchmark datasets."	"Pratik Mazumder, Pravendra Singh and Vinay P. Namboodiri,  “Improving Few-Shot Learning using Composite Rotation based Auxiliary Task”, <i> The IEEE Winter Conference on Applications of Computer Vision, WACV 2021</i>"	mazumder2020improving	https://arxiv.org/abs/2006.15919
2020-07-19	Passive Batch Injection Training Technique: Boosting Network Performance by Injecting Mini-Batches from a different Data Distribution	"International Joint Conference on Neural Networks (IJCNN), 2020"	"This work presents a novel training technique for deep neural networks that makes use of additional data from a distribution that is different from that of the original input data. This technique aims to reduce overfitting and improve the generalization performance of the network. Our proposed technique, namely Passive Batch Injection Training Technique (PBITT), even reduces the level of overfitting in networks that already use the standard techniques for reducing overfitting such as L2 regularization and batch normalization, resulting in significant accuracy improvements. Passive Batch Injection Training Technique (PBITT) introduces a few passive mini-batches into the training process that contain data from a distribution that is different from the input data distribution. This technique does not increase the number of parameters in the final model and also does not increase the inference (test) time but still improves the performance of deep CNNs. To the best of our knowledge, this is the first work that makes use of different data distribution to aid the training of convolutional neural networks (CNNs). We thoroughly evaluate the proposed approach on standard architectures: VGG, ResNet, and WideResNet, and on several popular datasets: CIFAR-10, CIFAR-100, SVHN, and ImageNet. We observe consistent accuracy improvement by using the proposed technique. We also show experimentally that the model trained by our technique generalizes well to other tasks such as object detection on the MS-COCO dataset using Faster R-CNN. We present extensive ablations to validate the proposed approach. Our approach improves the accuracy of VGG-16 by a significant margin of 2.1% over the CIFAR-100 dataset."	"P. Singh, P. Mazumder and V. P. Namboodiri, ""Passive Batch Injection Training Technique: Boosting Network Performance by Injecting Mini-Batches from a different Data Distribution,"" 2020 International Joint Conference on Neural Networks (IJCNN), Glasgow, United Kingdom, 2020, pp. 1-8, doi: 10.1109/IJCNN48605.2020.9206622."	singh2020passive	https://arxiv.org/abs/2006.04406
2020-03-01	Bridged Variational Autoencoders for Joint Modeling of Images and Attributes	IEEE Winter Conference on Applications of Computer Vision (WACV)	"Generative models have recently shown the ability to realistically generate data and model the distribution accurately. However, joint modeling of an image with the attribute that it is labeled with requires learning a cross modal correspondence between image and attribute data. Though the information present in a set of images and its attributes possesses completely different statistical properties altogether, there exists an inherent correspondence that is challenging to capture. Various models have aimed at capturing this correspondence either through joint modeling of a variational autoencoder or through separate encoder networks that are then concatenated. We present an alternative by proposing a bridged variational autoencoder that allows for learning cross-modal correspondence by incorporating cross-modal hallucination losses in the latent space. In comparison to the existing methods, we have found that by using a bridge connection in latent space we not only obtain better generation results, but also obtain highly parameter-efficient model which provide 40% reduction in training parameters for bimodal dataset and nearly 70% reduction for trimodal dataset. We validate the proposed method through comparison with state of the art methods and benchmarking on standard datasets."	"Ravindra Yadav, Ashish Sardana, Vinay P Namboodiri, Rajesh M Hegde. ""Bridged Variational Autoencoders for Joint Modeling of Images and Attributes."" Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2020, pp. 1479-1487"	2020_wacv_bridgedVAE	https://ieeexplore.ieee.org/abstract/document/9093565
2020-06-14	Learning Individual Speaking Styles for Accurate Lip to Speech Synthesis	Pattern Recognition Journal or IEEE Conference on Computer Vision and Pattern Recognition (CVPR)	"Humans involuntarily tend to infer parts of the conversation from lip movements when the speech is absent or corrupted by external noise. In this work, we explore the task of lip to speech synthesis, i.e., learning to generate natural speech given only the lip movements of a speaker. Acknowledging the importance of contextual and speaker-specific cues for accurate lip-reading, we take a different path from existing works. We focus on learning accurate lip sequences to speech mappings for individual speakers in unconstrained, large vocabulary settings. To this end, we collect and release a large-scale benchmark dataset, the first of its kind, specifically to train and evaluate the single-speaker lip to speech task in natural settings. We propose a novel approach with key design choices to achieve accurate, natural lip to speech synthesis in such unconstrained scenarios for the first time. Extensive evaluation using quantitative, qualitative metrics and human evaluation shows that our method is four times more intelligible than previous works in this space."	"K. R. Prajwal, R. Mukhopadhyay, V. P. Namboodiri and C. V. Jawahar, ""Learning Individual Speaking Styles for Accurate Lip to Speech Synthesis,"" 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Seattle, WA, USA, 2020, pp. 13793-13802, doi: 10.1109/CVPR42600.2020.01381."	Prajwal_2020_CVPR	https://arxiv.org/abs/2005.08209
2020-10-12	A Lip Sync Expert Is All You Need For Speech To Lip Generation In The Wild	"ACM International Conference on Multimedia, 2020 (ACM Multimedia)"	"In this work, we investigate the problem of lip-syncing a talking face video of an arbitrary identity to match a target speech segment. Current works excel at producing accurate lip movements on a static image or videos of specific people seen during the training phase. However, they fail to accurately morph the lip movements of arbitrary identities in dynamic, unconstrained talking face videos, resulting in significant parts of the video being out-of-sync with the new audio. We identify key reasons pertaining to this and hence resolve them by learning from a powerful lip-sync discriminator. Next, we propose new, rigorous evaluation benchmarks and metrics to accurately measure lip synchronization in unconstrained videos. Extensive quantitative evaluations on our challenging benchmarks show that the lip-sync accuracy of the videos generated by our Wav2Lip model is almost as good as real synced videos."	"K R Prajwal, Rudrabha Mukhopadhyay, Vinay P. Namboodiri, and C.V. Jawahar. 2020. A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild. In Proceedings of the 28th ACM International Conference on Multimedia (MM '20). Association for Computing Machinery, New York, NY, USA, 484–492. DOI:https://doi.org/10.1145/3394171.3413532"	Prajwal2020ALS	https://arxiv.org/abs/2008.10010
2021-01-05	Visual Speech Enhancement Without A Real Visual Stream	Winter Conference on Applications of Computer Vision (WACV ’21)	"In this work, we re-think the task of speech enhancement in unconstrained real-world environments. Current state-of-the-art methods use only the audio stream and are limited in their performance in a wide range of real-world noises. Recent works using lip movements as additional cues improve the quality of generated speech over ``audio-only"" methods. But, these methods cannot be used for several applications where the visual stream is unreliable or completely absent. We propose a new paradigm for speech enhancement by exploiting recent breakthroughs in speech-driven lip synthesis. Using one such model as a teacher network, we train a robust student network to produce accurate lip movements that mask away the noise, thus acting as a ``visual noise filter"". The intelligibility of the speech enhanced by our pseudo-lip approach is almost close (< 3\% difference) to the case of using real lips. This implies that we can exploit the advantages of using lip movements even in the absence of a real video stream. We rigorously evaluate our model using quantitative metrics as well as qualitative human evaluations. Additional ablation studies and a demo video in the supplementary material containing qualitative comparisons and results clearly illustrate the effectiveness of our approach."	"Hegde, Sindhu B., K. Prajwal, R. Mukhopadhyay, Vinay Namboodiri and C. Jawahar. “Visual Speech Enhancement Without A Real Visual Stream.” Winter Conference on Applications of Computer Vision (WACV ’21)"	2021_wacv_hegde	https://arxiv.org/abs/2012.10852
2021-01-06	Multimodal Humor Dataset: Predicting Laughter tracks for Sitcoms	The IEEE Winter Conference on Applications of Computer Vision	"A great number of situational comedies (sitcoms) are being regularly made and the task of adding laughter tracks to these is a critical task. Providing an ability to be able to predict whether something will be humorous to the audience is also crucial. In this project, we aim to automate this task. Towards doing so, we annotate an existing sitcom (`Big Bang Theory') and use the laughter cues present to obtain a manual annotation for this show. We provide detailed analysis for the dataset design and further evaluate various state of the art baselines for solving this task. We observe that existing LSTM and BERT based networks on the text alone do not perform as well as joint text and video or only video-based networks. Moreover, it is challenging to ascertain that the words attended to while predicting laughter are indeed humorous. Our dataset and analysis provided through this paper is a valuable resource towards solving this interesting semantic and practical task. As an additional contribution, we have developed a novel model for solving this task that is a multi-modal self-attention based model that outperforms currently prevalent models for solving this task. The project page for our paper is \url{https://delta-lab-iitk.github.io/Multimodal-Humor-Dataset/}."	"Badri N. Patro, Mayank Lunayach, Deepankar Srivastava, Sarvesh,  Hunar Singh, Vinay P. Namboodiri (2021). `` Multimodal Humor Dataset: Predicting Laughter tracks for Sitcoms'', The IEEE Winter Conference on Applications of Computer Vision, USA, 2021.)"	patro_wacv_2021	https://delta-lab-iitk.github.io/Multimodal-Humor-Dataset/
2021-01-05	Self Supervision for Attention Networks	The IEEE Winter Conference on Applications of Computer Vision	" In recent years, the attention mechanism has become a fairly popular concept and has proven to be successful in many machine learning applications. However, deep learning models do not employ supervision for these attention mechanisms which can improve the model's performance significantly. Therefore, in this paper, we tackle this limitation and propose a novel method to improve the attention mechanism by inducing ``self-supervision"". We devise a technique to generate desirable attention maps for any model that utilizes an attention module. This is achieved by examining the model's output for different regions sampled from the input and obtaining the attention probability distributions that enhance the proficiency of the model. The attention distributions thus obtained are used for supervision. We rely on the fact, that attenuation of the unimportant parts, allows a model to attend to more salient regions, thus strengthening the prediction accuracy. The quantitative and qualitative results published in this paper show that this method successfully improves the attention mechanism as well as the model's accuracy. In addition to the task of Visual Question Answering(VQA), we also show results on the task of Image classification and Text classification to prove that our method can be generalized to any vision and language model that uses an attention module."	"Badri N. Patro, Kasturi G S, Ansh Jain, Vinay P. Namboodiri (2021). `` Self Supervision for Attention Networks '', The IEEE Winter Conference on Applications of Computer Vision, USA, 2021.)"	patro_wacv_2021	https://github.com/Anonymous1207/Self_Supervsion_for_Attention_Networks
2020-03-01	Deep Bayesian Network for Visual Question Generation	The IEEE Winter Conference on Applications of Computer Vision	"Generating natural questions from an image is a semantic task that requires using vision and language modalities to learn multimodal representations. Images can have multiple visual and language cues such as places, captions, and tags. In this paper, we propose a principled deep Bayesian learning framework that combines these cues to produce natural questions. We observe that with the addition of more cues and by minimizing uncertainty in the among cues, the Bayesian network becomes more confident. We propose a Minimizing Uncertainty of Mixture of Cues (MUMC), that minimizes uncertainty present in a mixture of cues experts for generating probabilistic questions. This is a Bayesian framework and the results show a remarkable similarity to natural questions as validated by a human study. We observe that with the addition of more cues and by minimizing uncertainty among the cues, the Bayesian framework becomes more confident. Ablation studies of our model indicate that a subset of cues is inferior at this task and hence the principled fusion of cues is preferred. Further, we observe that the proposed approach substantially improves over state-of-the-art benchmarks on the quantitative metrics (BLEU-n, METEOR, ROUGE, and CIDEr). Here we provide project link for Deep Bayesian VQG https://delta-lab-iitk. github. io/BVQG/."	"Patro, B., Kurmi, V., Kumar, S., & Namboodiri, V. (2020). Deep Bayesian Network for Visual Question Generation. In The IEEE Winter Conference on Applications of Computer Vision (pp. 1566-1576)."	patro2020deep	https://openaccess.thecvf.com/content_WACV_2020/html/Patro_Deep_Bayesian_Network_for_Visual_Question_Generation_WACV_2020_paper.html
2020-03-01	Robust Explanations for Visual Question Answering	The IEEE Winter Conference on Applications of Computer Vision	"In this paper, we propose a method to obtain robust explanations for visual question answering (VQA) that correlate well with the answers. Our model explains the answers obtained through a VQA model by providing visual and textual explanations. The main challenges that we address are i) Answers and textual explanations obtained by current methods are not well correlated and ii) Current methods for visual explanation do not focus on the right location for explaining the answer. We address both these challenges by using a collaborative correlated module which ensures that even if we do not train for noise based attacks, the enhanced correlation ensures that the right explanation and answer can be generated. We further show that this also aids in improving the generated visual and textual explanations. The use of the correlated module can be thought of as a robust method to verify if the answer and explanations are coherent. We evaluate this model using VQA-X dataset. We observe that the proposed method yields better textual and visual justification that supports the decision. We showcase the robustness of the model against a noise-based perturbation attack using corresponding visual and textual explanations. A detailed empirical analysis is shown."	"Patro, B., Patel, S., & Namboodiri, V. (2020). Robust Explanations for Visual Question Answering. In The IEEE Winter Conference on Applications of Computer Vision (pp. 1577-1586)."	patro_wacv_2020_robust	https://openaccess.thecvf.com/content_WACV_2020/papers/Patro_Robust_Explanations_for_Visual_Question_Answering_WACV_2020_paper.pdf
2020-02-07	Explanation vs Attention: A Two-Player Game to Obtain Attention for VQA	 Association for the Advancement of Artificial Intelligence	"In this paper, we aim to obtain improved attention for a visual question answering (VQA) task. It is challenging to provide supervision for attention. An observation we make is that visual explanations as obtained through class activation mappings (specifically Grad-CAM) that are meant to explain the performance of various networks could form a means of supervision. However, as the distributions of attention maps and that of Grad-CAMs differ, it would not be suitable to directly use these as a form of supervision. Rather, we propose the use of a discriminator that aims to distinguish samples of visual explanation and attention maps. The use of adversarial training of the attention regions as a two-player game between attention and explanation serves to bring the distributions of attention maps and visual explanations closer. Significantly, we observe that providing such a means of supervision also results in attention maps that are more closely related to human attention resulting in a substantial improvement over baseline stacked attention network (SAN) models. It also results in a good improvement in rank correlation metric on the VQA task. This method can also be combined with recent MCB based methods and results in consistent improvement. We also provide comparisons with other means for learning distributions such as based on Correlation Alignment (Coral), Maximum Mean Discrepancy (MMD) and Mean Square Error (MSE) losses and observe that the adversarial loss outperforms the other forms of learning the attention maps. Visualization of the results also confirms our hypothesis that attention maps improve using this form of supervision."	"Patro, B., Anupriy & Namboodiri, V. (2020, April). Explanation vs attention: A two-player game to obtain attention for vqa. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 34, No. 07, pp. 11848-11855)."	patro_wacv_2020_explanation	"https://aaai.org/ojs/index.php/AAAI/article/view/6858/6712, AAAI-2020"
2020-10-12	Visually Precise Query	Proceedings of the 28th ACM International Conference on Multimedia	"We present the problem of Visually Precise Query (VPQ) generation which enables a more intuitive match between a user's information need and an e-commerce site's product description. Given an image of a fashion item, what is the most optimum search query that will retrieve the exact same or closely related product(s) with high probability. In this paper we introduce the task of VPQ generation which takes a product image and its title as its input and provides aword level extractive summary of the title, containing a list of salient attributes, which can now be used as a query to search for similar products. We collect a large dataset of fashion images and their titles and merge it with an existing research dataset which was created for a different task. Given the image and title pair, VPQ problem is posed as identifying a non-contiguous collection of spans within the title. We provide a dataset of around 400K image, title and corresponding VPQ entries and release it to the research community. We provide a detailed description of the data collection process as well as discuss the future direction of research for the problem introduced in this work. We provide the standard text as well as visual domain baseline comparisons and also provide multi-modal baseline models to analyze the task introduced in this work. Finally, we propose a hybrid fusion model which promises to be the direction of research in the multi-modal community."	"Dasgupta, Riddhiman and Tom, Francis and Kumar, Sudhir and Das Gupta, Mithun and Kumar, Yokesh and Patro, Badri N. and Namboodiri, Vinay P. (2020)``Visually Precise Query'', Proceedings of the 28th ACM International Conference on Multimedia, New York, NY, USA, 2020."	dasgupta_acm_2021_visual	https://dl.acm.org/doi/abs/10.1145/3394171.3413558#d81339e1
2020-11-09	Uncertainty Class Activation Map (U-CAM) using Gradient Certainty method	 IEEE Transactions on Image Processing	"Understanding and explaining deep learning models is an imperative task. Towards this, we propose a method that obtains gradient-based certainty estimates that also provide visual attention maps. Particularly, we solve for visual question answering task. We incorporate modern probabilistic deep learning methods that we further improve by using the gradients for these estimates. These have two-fold benefits: a) improvement in obtaining the certainty estimates that correlate better with misclassified samples and b) improved attention maps that provide state-of-the-art results in terms of correlation with human attention regions. The improved attention maps result in consistent improvement for various methods for visual question answering. Therefore, the proposed technique can be thought of as a tool for obtaining improved certainty estimates and explanations for deep learning models. We provide detailed empirical analysis for the visual question answering task on all standard benchmarks and comparison with state of the art methods."	"Badri N Patro, Mayank Lunayach, Vinay P Namboodiri (2020).``Uncertainty Class Activation Map (U-CAM) using Gradient Certainty method'', IEEE Transactions on Image Processing,2020."	patro_TIP_2020_uncertainty	https://arxiv.org/pdf/2002.10309.pdf
2020-08-11	Revisiting paraphrase question generator using pairwise discriminator	Neurocomputing	"In this paper, we propose a method for obtaining sentence-level embeddings. While the problem of obtaining word-level embeddings is very well studied, we propose a novel method for obtaining sentence-level embeddings. This is obtained by a simple method in the context of solving the paraphrase generation task. If we use a sequential encoder-decoder model for generating paraphrase, we would like the generated paraphrase to be semantically close to the original sentence. One way to ensure this is by adding constraints for true paraphrase embeddings to be close and unrelated paraphrase candidate sentence embeddings to be far. This is ensured by using a sequential pair-wise discriminator that shares weights with the encoder. This discriminator is trained with a suitable loss function. Our loss function penalizes paraphrase sentence embedding distances from being too large. This loss is used in combination with a sequential encoder-decoder network. We also validate our method by evaluating the obtained embeddings for a sentiment analysis task. The proposed method results in semantic embeddings and provide competitive results on the paraphrase generation and sentiment analysis task on standard dataset. These results are also shown to be statistically significant."	"Badri N Patro, Dev Chauhan, Vinod K Kurmi, Vinay P Namboodiri,(2020)``Revisiting paraphrase question generator using pairwise discriminator'',Neurocomputing, Volume 420, 2021, pp. 149-161"	patro_neurocomputing_2020_pair	https://www.sciencedirect.com/science/article/abs/pii/S0925231220312820
2020-08-09	 Probabilistic framework for solving visual dialog	Pattern Recognition 	"In this paper, we propose a probabilistic framework for solving the task of ‘Visual Dialog’. Solving this task requires reasoning and understanding of visual modality, language modality, and common sense knowledge to answer. Various architectures have been proposed to solve this task by variants of multi-modal deep learning techniques that combine visual and language representations. However, we believe that it is crucial to understand and analyze the sources of uncertainty for solving this task. Our approach allows for estimating uncertainty and also aids a diverse generation of answers. The proposed approach is obtained through a probabilistic representation module that provides us with representations for image, question and conversation history, a module that ensures that diverse latent representations for candidate answers are obtained given the probabilistic representations and an uncertainty representation module that chooses the appropriate answer that minimizes uncertainty. We thoroughly evaluate the model with a detailed ablation analysis, comparison with state of the art and visualization of the uncertainty that aids in the understanding of the method. Using the proposed probabilistic framework, we thus obtain an improved visual dialog system that is also more explainable."	"Badri N. Patro, Anupriy & Vinay P. Namboodiri, ``Probabilistic framework for solving visual dialog'', Pattern Recognition, Volume 110, February 2021, 107586."	patro_PR_2020_probabilistic	https://www.sciencedirect.com/science/article/abs/pii/S0031320320303897
2020-09-07	Determinantal Point Process as an alternative to NMS	British Machine Vision Conference (BMVC)	"We present a determinantal point process (DPP) inspired alternative to non-maximumsuppression (NMS) which has become an integral step in all state-of-the-art object de-tection frameworks.  DPPs have been shown to encourage diversity in subset selectionproblems. We pose NMS as a subset selection problem and posit that directly incor-porating DPP like framework can improve the overall performance of the object detectionsystem.  We propose an optimization problem which takes the same inputs as NMS, butintroduces a novel sub-modularity based diverse subset selection functional. Our resultsstrongly indicate that the modifications proposed in this paper can provide consistentimprovements to state-of-the-art object detection pipelines."	"Some, Samik, Mithun Das Gupta, and Vinay P. Namboodiri. ""Determinantal Point Process as an alternative to NMS."" Proceedings of British Machine Vision Conference (2020), arXiv preprint arXiv:2008.11451 (2020)."	some2020determinantal	https://arxiv.org/abs/2008.11451
2020-09-07	SD-MTCNN: Self-Distilled Multi-Task	British Machine Vision Conference (BMVC)	"Multi-task learning (MTL) using convolutional neural networks (CNN) deals with training the network for multiple correlated tasks in concert. For accuracy-critical applications, there are endeavors to boost the model performance by resorting to a deeper network, which also increases the model complexity. However, such burdensome models are difficult to be deployed on mobile or edge devices. To ensure a trade-off between performance and complexity of CNNs in the context of MTL, we introduce the novel paradigm of self-distillation within the network. Different from traditional knowledge distillation (KD), which trains the Student in accordance with a cumbersome Teacher, our self-distilled multi-task CNN model: SD-MTCNN aims at distilling knowledge from deeper CNN layers into the shallow layers. Precisely, we follow a hard-sharing based MTL setup where all the tasks share a generic feature-encoder on top of which separate task-specific decoders are enacted. Under this premise, SD-MTCNN distills the more abstract features from the decoders to the encoded feature space, which guarantees improved multi-task performance from different parts of the network. We validate SD-MTCNN on three benchmark datasets: CityScapes, NYUv2, and Mini-Taskonomy, and results confirm the improved generalization capability of self-distilled multi-task CNNs in comparison to the literature and baselines"	"Ankit Jha, Awanish Kumar, Biplab Banerjee and Vinay Namboodiri, “SD-MTCNN: Self-Distilled Multi-Task CNN”, Proceedings of British Machine Vision Conference (2020),"	jha_bmvc2020	https://www.bmvc2020-conference.com/conference/papers/paper_0448.html
2020-03-01	 Jointly Trained Image and Video Generation using Residual Vectors	2020 IEEE Winter Conference on Applications of Computer Vision (WACV)	"In this work, we propose a modeling technique for jointly training image and video generation models by simultaneously learning to map latent variables with a fixed prior onto real images and interpolate over images to generate videos. The proposed approach models the variations in representations using residual vectors encoding the change at each time step over a summary vector for the entire video. We utilize the technique to jointly train an image generation model with a fixed prior along with a video generation model lacking constraints such as disentanglement. The joint training enables the image generator to exploit temporal information while the video generation model learns to flexibly share information across frames. Moreover, experimental results verify our approach's compatibility with pre-training on videos or images and training on datasets containing a mixture of both. A comprehensive set of quantitative and qualitative evaluations reveal the improvements in sample quality and diversity over both video generation and image generation baselines. We further demonstrate the technique's capabilities of exploiting similarity in features across frames by applying it to a model based on decomposing the video into motion and content. The proposed model allows minor variations in content across frames while maintaining the temporal dependence through latent vectors encoding the pose or motion features."	"Yatin Dandi, Aniket Das, Soumye Singhal, Vinay Namboodiri, Piyush Rai; “Jointly Trained Image and Video Generation using Residual Vectors “, <i>Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2020, pp. 3028-3042 </i>"	dandi_wacv2020	https://openaccess.thecvf.com/content_WACV_2020/html/Dandi_Jointly_Trained_Image_and_Video_Generation_using_Residual_Vectors_WACV_2020_paper.html
2020-12-06	STEER: Simple Temporal Regularization For Neural ODEs	2020 Neural Information Processing Systems Conference (NeuRIPS)	"Training Neural Ordinary Differential Equations (ODEs) is often computationally expensive. Indeed, computing the forward pass of such models involves solving an ODE which can become arbitrarily complex during training. Recent works have shown that regularizing the dynamics of the ODE can partially alleviate this. In this paper we propose a new regularization technique: randomly sampling the end time of the ODE during training. The proposed regularization is simple to implement, has negligible overhead and is effective across a wide variety of tasks. Further, the technique is orthogonal to several other methods proposed to regularize the dynamics of ODEs and as such can be used in conjunction with them. We show through experiments on normalizing flows, time series models and image recognition that the proposed regularization can significantly decrease training time and even improve performance over baseline models."	"Arnab Ghosh, Harkirat Singh Behl, Emilien Dupont, Philip H. S. Torr, Vinay Namboodiri, “STEER: Simple Temporal Regularization For Neural ODEs”, <i> Proceedings of Neural Information Processing Systems Conference (NeuRIPS) 2020 </i>"	ghosh_neurips2020	https://arxiv.org/abs/2006.10711
2020-07-15	A Multilingual Parallel Corpora Collection Effort for Indian Languages	Proceedings of The 12th Language Resources and Evaluation Conference (LREC)	"We present sentence aligned parallel corpora across 10 Indian Languages-Hindi, Telugu, Tamil, Malayalam, Gujarati, Urdu, Bengali, Oriya, Marathi, Punjabi, and English-many of which are categorized as low resource. The corpora are compiled from online sources which have content shared across languages. The corpora presented significantly extends present resources that are either not large enough or are restricted to a specific domain (such as health). We also provide a separate test corpus compiled from an independent online source that can be independently used for validating the performance in 10 Indian languages. Alongside, we report on the methods of constructing such corpora using tools enabled by recent advances in machine translation and cross-lingual retrieval using deep neural network based methods."	"Shashank Siripragada, Jerin Philip, Vinay P Namboodiri, CV Jawahar, “A Multilingual Parallel Corpora Collection Effort for Indian Languages”, <i>Proceedings of The 12th Language Resources and Evaluation Conference</i>"	siripragada_lrec2020	https://arxiv.org/abs/2007.07691
2021-01-05	Revisiting Low Resource Status of Indian Languages in Machine Translation	ACM India Joint International Conference on Data Science & Management of Data (CODS-COMAD) 2021	"Indian language machine translation performance is hampered due to the lack of large scale multi-lingual sentence aligned corpora and robust benchmarks. Through this paper, we provide and analyse an automated framework to obtain such a corpus for Indian language neural machine translation (NMT) systems. Our pipeline consists of a baseline NMT system, a retrieval module, and an alignment module that is used to work with publicly available websites such as press releases by the government. The main contribution towards this effort is to obtain an incremental method that uses the above pipeline to iteratively improve the size of the corpus as well as improve each of the components of our system. Through our work, we also evaluate the design choices such as the choice of pivoting language and the effect of iterative incremental increase in corpus size. Our work in addition to providing an automated framework also results in generating a relatively larger corpus as compared to existing corpora that are available for Indian languages. This corpus helps us obtain substantially improved results on the publicly available WAT evaluation benchmark and other standard evaluation benchmarks. 
"	"Jerin Philip, Shashank Siripragada, Vinay P Namboodiri, CV Jawahar, “Revisiting Low Resource Status of Indian Languages in Machine Translation”, <i>8th ACM IKDD CODS and 26th COMAD (CODS COMAD 2021), January 2-4, 2021, Bangalore, India</i>"	philip_codscomad2021	https://arxiv.org/abs/2008.04860
2020-10-25	Learning to Switch CNNs with Model Agnostic Meta Learning for Fine Precision Visual Servoing	IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS-2020)	"Convolutional Neural Networks (CNNs) have been successfully applied for relative camera pose estimation from labeled image-pair data, without requiring any hand-engineered features, camera intrinsic parameters or depth information. The trained CNN can be utilized for performing pose based visual servo control (PBVS). One of the ways to improve the quality of visual servo output is to improve the accuracy of the CNN for estimating the relative pose estimation. With a given state-of-the-art CNN for relative pose regression, how can we achieve an improved performance for visual servo control? In this paper, we explore switching of CNNs to improve the precision of visual servo control. The idea of switching a CNN is due to the fact that the dataset for training a relative camera pose regressor for visual servo control must contain variations in relative pose ranging from a very small scale to eventually a larger scale. We found that, training two different instances of the CNN, one for large-scale-displacements (LSD) and another for small-scale-displacements (SSD) and switching them during the visual servo execution yields better results than training a single CNN with the combined LSD+SSD data. However, it causes extra storage overhead and switching decision is taken by a manually set threshold which may not be optimal for all the scenes. To eliminate these drawbacks, we propose an efficient switching strategy based on model agnostic meta learning (MAML) algorithm. In this, a single model is trained to learn parameters which are simultaneously good for multiple tasks, namely a binary classification for switching decision, a 6DOF pose regression for LSD data and also a 6DOF pose regression for SSD data. The proposed approach performs far better than the naive approach, while storage and run-time overheads are almost negligible. 
"	"Prem Raj, Vinay P. Namboodiri, Laxmidhar Behera, “Learning to Switch CNNs with Model Agnostic Meta Learning for Fine Precision Visual Servoing”, <i>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS-2020)</i>"	raj_iros2020	https://arxiv.org/abs/2007.04645