---
title: "Audio - Visual Humans"
excerpt: "Project page for Audio Visual Humans"
collection: portfolio
---

## Audio-Visual Digital Humans

We have been pursuing research on a number of aspects related to audio-visual digital humans. On this page we present the works that we have been involved with

### Wav2Lip: Accurately Lip-syncing Videos In The Wild
In this work, we investigate the problem of lip-syncing a talking face video of an arbitrary identity to match a target speech segment. Current works excel at producing accurate lip movements on a static image or on videos of specific people seen during the training phase. However, they fail to accurately morph the actual lip movements of arbitrary identities in dynamic, unconstrained talking face videos, resulting in significant parts of the video being out-of-sync with the newly chosen audio. We identify key reasons pertaining to this and hence resolve them by learning from a powerful lip-sync discriminator. Next, we propose new, rigorous evaluation benchmarks and metrics to specifically measure the accuracy of lip synchronization in unconstrained videos. Extensive quantitative and human evaluations on our challenging benchmarks show that the lip-sync accuracy of the videos generated using our Wav2Lip model is almost as good as real synced videos. We clearly demonstrate the substantial impact of our Wav2Lip model in our publicly available demo video. We also open-source our code, models, and evaluation benchmarks to promote future research efforts in this space.

[Detailed Project page](http://cvit.iiit.ac.in/research/projects/cvit-projects/a-lip-sync-expert-is-all-you-need-for-speech-to-lip-generation-in-the-wild/)

[![Watch the video](https://img.youtube.com/vi/0fXaDCZNOJc/maxresdefault.jpg)]( https://www.youtube.com/watch?v=0fXaDCZNOJc)


### Lip2Wav: Audio generation based on lip movements 
Humans involuntarily tend to infer parts of the conversation from lip movements when the speech is absent or corrupted by external noise. In this work, we explore the task of lip to speech synthesis, i.e., learning to generate natural speech given only the lip movements of a speaker. Acknowledging the importance of contextual and speaker-specific cues for accurate lip reading, we take a different path from existing works. We focus on learning accurate lip sequences to speech mappings for individual speakers in unconstrained, large vocabulary settings. To this end, we collect and release a large-scale benchmark dataset, the first of its kind, specifically to train and evaluate the single-speaker lip to speech task in natural settings. We propose an approach to achieve accurate, natural lip to speech synthesis in such unconstrained scenarios for the first time. Extensive evaluation using quantitative, qualitative metrics and human evaluation shows that our method is almost twice as intelligible as previous works in this space.

[Detailed Project page](https://cvit.iiit.ac.in/research/projects/cvit-projects/speaking-by-observing-lip-movements)

[![Watch the video](https://img.youtube.com/vi/0HziA-jmlk_4/maxresdefault.jpg)]( https://www.youtube.com/watch?v=HziA-jmlk_4)

### Face to Face Translation
